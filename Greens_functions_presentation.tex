% rubber: set program xelatex

% The theme used for this presentation is matze's mtheme, which can be
% found at https://github.com/matze/mtheme

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THINGS TO TALK ABOUT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% Question: Green's function is NOT unique, so can we really treat it as
%           THE response to a source?
%           - Green's function is unique, given specified BC/IC's
%           - So we could say it's the response to the source, under the
%             the imposed BC/IC's
%           - But does this weaken the interpretation of BC/IC's as sources?
%           - Not really: It's the homogeneous BC's that make the Green's function unique
%
%
% Direct solution
%   - Discontinuity condition
%
% Eigenvalue solution
%
% Boundary conditions
%   - Green's function not arbitrary! Depends on BC/IC's.
%   - Idea of replacing BC/IC's with equivalent sources (Morse and Feshback)
%   - Relation to equivalence principle?
%   - Can also just find any Green's and add homogeneous solutions
%   - Can also solve adjoint Green's function problem
%
% Interpreting the Green's function
%   - Space-time invariance
%   - Self-adjointness (IC problems are not self-adjoint!)
%   - Reciprocity (weird combination of invariance/self-adjointness?)
%   - Poles are eigenvalues
%
% Wave equation causality
%   - Causal Green's functions comes from initial conditions in time domain
%   - Equivalent to assuming small loss (a la Harrington)
%   - Can replace initial conditions with equivalent sources
%   - Fourier transform actually fails here!
% 
% Scattering
%   - Interesting approximation technique (Born): the field itself is the source
% Perturbation theory?
% Huygens principle and the propagator?
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newif\ifhandout
\newif\ifextended

\handoutfalse
\handouttrue

\extendedfalse
%\extendedtrue

\ifhandout
    \documentclass[12 pt, compress, handout, intlimits]{beamer}

    \setbeamertemplate{note page}[plain]

    \setbeameroption{show notes}% on second screen=bottom}
\else

    \documentclass[12 pt, compress, intlimits]{beamer}

\fi

\usetheme{metropolis}

\usepackage{mymacros}
\usepackage[retainorgcmds]{IEEEtrantools}
\setlength{\IEEEnormaljot}{9pt}

\renewcommand{\d}{\operatorname{d}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\B}{\mathcal{B}}
\newcommand{\inprod}[2]{\left\langle {#1}, {#2} \right\rangle}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{array}
\usepackage{tabularx}

\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{empheq}

\renewcommand{\tabularxcolumn}[1]{>{\normalsize}m{#1}}

\newcommand{\highlight}[1]{\colorbox{mLightBrown!65}{$\displaystyle{#1}$}}
\newcommand{\mygreenbox}[1]{\colorbox{mLightBrown!65}{\hspace{1em}#1\hspace{1em}}}
\newcommand*\widefbox[1]{\fbox{\hspace{1em}#1\hspace{1em}}}

%\usepackage{sourcesanspro}

%\usepgfplotslibrary{dateplot}

%\usefonttheme[onlymath]{serif}
%\usepackage{eulervm}
%\usepackage{arevmath}
%\renewcommand{\vect}[1]{\vec{#1}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\ft}[1]{\tilde{#1}}
\newcommand{\rinprod}[2]{\left\langle {#1}, {#2} \right\rangle_r}

\useinnertheme{circles}

\setbeamercovered{transparent}

\title{Green's functions}
\subtitle{A short introduction}
\date{\today}
\author{Chris Deimert}
\institute{Department of Electrical and Computer Engineering, University of Calgary}

\begin{document}

\maketitle

\note{
    \begin{itemize}
    \item
        This is intended as a short introduction to Green's functions for electrical engineers.
    \item
        Basic idea of Green's functions is simple, but there is a huge amount of theory for actually calculating and using them.
    \item
        For a short presentation, we can either cover a lot of material incompletely, or a small amount of material thoroughly.
        The second is chosen here, because I think it leads to a clearer understanding of Green's functions as a whole.
        Of course, the downside is that we'll miss out on a lot of interesting topics and applications.
    \item
        Suggested further reading provided at the end.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\note{
\begin{enumerate}
\item
    Basic idea of Green's functions.
\item
    Simplest method for solving the Green's function equation.
\item
    How to use the Green's function to solve a problem with boundary conditions. (Biggest section!)
\item
    Useful properties of Green's functions for special types of problems.
\item
    Relationship between Green's functions and eigenvalues/eigenfunctions.
\item
    Summary and suggested further reading.
\end{enumerate}

}

\section{Introduction}
\label{sec:introduction}

\note{
\begin{itemize}
\item
    The basic idea of Green's functions is really simple.
    You've actually used them before!
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{What is a Green's function?}
    
    Linear equation to solve:
    \begin{align*}
        \L u(x) &= f(x)
    \end{align*}
    
    \pause

    Green's function is the \textbf{impulse response}:
    \begin{align*}
        \L G(x,x') &= \delta(x - x')
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Most EM problems are described by linear (differential) equations with some source/driving function $ f(x) $.
\item
    The Green's function is the solution when the source $ f(x) $ is an impulse located at $ x' $.
\item
    Can think of it as a generalization of the impulse response from signal processing.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Why is it useful?}
    
    \begin{align*}
        \delta(x - x') &\xrightarrow{\quad \L^{-1} \quad} G(x,x')
    \end{align*}
 
    \pause

    \begin{align*}
        f(x) = \int \delta(x - x') f(x') \d x \xrightarrow{\quad \L^{-1} \quad} \int G(x,x') f(x') \d x
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Once we know the Green's function for a problem, we can find the solution for any source $ f(x) $.
\item
    Impulses $ \delta(x - x') $ produce a response $ G(x,x') $.
\item
    We can split the source $ f(x) $ up into a sum (integral) of impulses $ \delta(x - x') $.
\item
    Then the response to $ f(x) $ is just a weighted sum (integral) of impulse responses.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Why is it useful?}

    \begin{align*}
        \L u(x) &= f(x)
    \end{align*}
    \begin{align*}
        \L G(x,x') &= \delta(x - x')
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        u(x) = \int G(x,x') f(x') \d x
    \end{empheq}
    \begin{flushright}\scriptsize{(Some conditions apply.)}\end{flushright}

\end{frame}

\note{
\begin{itemize}
\item
    Once we know the Green's function, we have an explicit formula for the solution $ u(x) $ for any source function $ f(x) $.
\item
    Beware the fine print! 
    This formula actually only works under certain assumptions about the boundary conditions.
\item
    We'll deal with the more general approach later.
    For now, this gets the key idea across.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}

    Impulse response of an LTI system:
    \begin{align*}
        y(t) &= \int_{-\infty}^{\infty} x(t') \alert{h(t - t')} \d t'
    \end{align*}

    %E.g., for an RL-circuit:
    %\begin{align*}
    %    G(t,t') &= h(t - t') = u(t - t') e^{-\alpha (t - t')}
    %\end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    In electrical engineering, we've seen Green's functions before.
\item
    Impulse response $ h(t - t') $ from linear system theory is an example of a Green's function.
    \begin{align*}
        G(t,t') & = h(t - t')
    \end{align*}
\item
    Usually find $ h(t - t') $ using Fourier transform of the transfer function.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}

    Poisson's equation:
    \begin{align*}
        \nabla^2 V(\vect{r}) &= - \frac{\rho(\vect{r})}{\epsilon_0}
    \end{align*}
    \begin{align*}
        V(\vect{r}) &= \iiint \alert{\frac{1}{4 \pi \epsilon_0 \left| \vect{r} - \vect{r}' \right|}} \rho(\vect{r}') \d^3 \vect{r}'
    \end{align*}
\end{frame}

\note{
    \begin{itemize}
    \item
        Green's function for Poisson's equation is
        \begin{align*}
            G(\vect{r}, \vect{r}') &= \frac{1}{4 \pi \epsilon_0 \left| \vect{r} - \vect{r}' \right|}
        \end{align*}
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}
    
    Helmholtz equation:
    \begin{align*}
        \left( \nabla^2 + k^2 \right) A_z(\vect{r}) &= -J_z(\vect{r})
    \end{align*}
    \begin{align*}
        A_z(\vect{r}) &= \iiint \alert{\frac{e^{-jk \left| \vect{r} - \vect{r}' \right|}}{4 \pi \left| \vect{r} - \vect{r}' \right|}} J_z\left( \vect{r}' \right) \d^3 \vect{r}'
    \end{align*}
\end{frame}

\note{
    \begin{itemize}
    \item
        Green's function for the Helmholtz equation is
        \begin{align*}
            G(\vect{r}, \vect{r}') &= \frac{e^{-jk \left| \vect{r} - \vect{r}' \right|}}{4 \pi \left| \vect{r} - \vect{r}' \right|}
        \end{align*}
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}
    
    Green's functions let us:
    \begin{itemize}
    \item
        Derive these expressions.
    \item
        Generalize to other problems and boundary conditions.
    \end{itemize}
    
\end{frame}

\note{
\begin{itemize}
\item
    With Green's function theory, we learn how to derive the above expressions. (Though we won't have time to do the 3D ones here.)
\item
    More importantly, Green's function theory allows us to deal with different boundary conditions.
    The solutions to the Poisson and Helmholtz equations above assume free space (boundaries at infinity).
    Green's functions would allow us to, e.g., find the response to a current source inside a specific waveguide.
\end{itemize}
}

\ifextended
\section{Generalized functions}
\label{sec:generalized_functions}

\note{
    \begin{itemize}
    \item
        Delta functions play a key role in Green's functions (and electrical engineering in general), but tend to lead to hand-waving.
    \item
        Worth seeing how they can be rigorously defined before moving on.
    \item
        Machinery for this is Schwartz's theory of distributions (generalized functions).
    \item
        See Folland (1992), \emph{Fourier analysis and its applications}, Chapter 9 for more.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Typical delta function definition}
    
    Typical ``definition'' of $ \delta(x - x_0) $:
    \begin{align*}
        \delta(x-x_0) &= 0 \quad \text{for} \quad x \neq x_0
    \end{align*}
    \begin{align*}
        \int_{-\infty}^{\infty} \delta(x-x_0) &= 1
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Often see definitions like this one.
\item
    Often said to imply that $ \delta(x - x_0) = \infty $ at $ x = x_0 $.
\item
    Might be okay intuitively, but very imprecise mathematically.
\item
    There is no true function which satisfies both of these requirements!
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Generalized functions}
    
    $ f(x) $ defines a linear operator $ \phi(x) $ via
    \begin{align*}
        f[\phi] &= \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Let's see if we can generalize the idea of a ``function'' so that it includes delta functions.
\item
    Given a function $ f(x) $, we can use it to define a linear operator (a functional, to be exact) on other functions $ \phi(x) $.
\item
    $ f[\cdot] $ is a linear operator. It takes a function $ \phi(x) $ and returns the number
    \begin{align*}
        f[\phi] &= \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
\item
    If we ensure that $ \phi(x) $ is very well-behaved, then every function $ f(x) $ defines an operator in this way.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Generalized functions}

    If we have $ f[\phi] $, but no $ f(x) $, then $ f $ is a generalized function.
    
    \textbf{Symbolically}, we write
    \begin{align*}
        f[\phi] &\stackrel{s}{=} \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        It's possible to have an operator $ f[\phi] $, but we can't find an $ f(x) $ to implement it via an integral.
    \item
        Then $ f(x) $ is a generalized function. It is not a function in its own right, but it is defined purely by its action on other functions $ f[\phi] $.
    \item
        We still symbolically write
        \begin{align*}
            f[\phi] &\stackrel{s}{=} \int_{-\infty}^{\infty} f(x) \phi(x) \d x
        \end{align*}
        but this just suggestive notation. 
        It is not actually an integral unless $ f(x) $ is a ``proper'' function!
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Defining the delta function}

    $ \delta(x-x_0) $ is a generalized function defined by the sifting property
    \begin{align*}
        \delta_{x_0}[\phi] &= \phi(x_0) \stackrel{s}{=} \int_{-\infty}^{\infty} \delta(x - x_0) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We can define a simple linear operator via the sifting property $ \delta_{x_0}[\phi] = \phi(x_0) $.
\item
    There is no actual function $ \delta(x - x_0) $ which gives
    \begin{align*}
        \int_{-\infty}^{\infty} \delta(x - x_0) \phi(x) \d x &= \phi(x_0)
    \end{align*}
    so $ \delta(x - x_0) $ is a generalized function and the above integral is purely symbolic.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function derivatives}
    
    We can define derivatives too:
    \begin{align*}
        \delta^{(n)}_{x_0}[\phi] &= (-1)^n \phi^{(n)}(x_0) \stackrel{s}{=} \int_{-\infty}^{\infty} \delta^{(n)}(x - x_0) \phi(x) \d x
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Generalized function theory lets us make sense of the derivatives of the delta function too.
\item
    $ \delta_{x_0}^{(n)} $ is just an operator that picks out the value of the $ n $th derivative of $ \phi(x) $ at the point $ x_0 $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function limits}
    
    \begin{align*}
        \lim_{\epsilon \to 0} f_\epsilon(x) &= \delta(x)
    \end{align*}
    if and only if
    \begin{align*}
        \lim_{\epsilon \to 0} f_\epsilon[\phi] = \lim_{\epsilon \to 0} \int_{-\infty}^{\infty} f_\epsilon(x) \phi(x) \d x &= \phi(0)
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Often useful to show that some set of actual functions $ f_\epsilon(x) $ ``approach'' the delta function in a limit.
\item
    To do this, we need to show that the sifting property is obeyed in the limit.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function limits}
    Limit of Gaussian functions:
    \begin{align*}
        \delta(x) &= \lim_{\epsilon \to 0} \frac{1}{\sqrt{2 \pi} \epsilon} e^{- x^2/ 2 \epsilon^2}
    \end{align*}
    Limit of Lorentzian functions:
    \begin{align*}
        \delta(x) &= \lim_{\epsilon \to 0} \frac{1}{\pi} \frac{\epsilon}{t^2 + \epsilon^2}
    \end{align*}
\end{frame}

\note{
\begin{itemize}
\item
    Two examples of delta function limits.
\item
    Confirms our intuition of the delta function as a limit of sharply-peaked functions.
\item
    In fact, basically any limit of sharply-peaked functions of area 1 will work: see Folland (Theorem 9.2).
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function limits}
    A more interesting example:
    \begin{align*}
        \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{j x t} \d t &= \delta(x)
    \end{align*}
    because
    \begin{align*}
        \lim_{\epsilon \to 0} \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{-\epsilon^2 t^2} e^{j x t} \d t = \delta(x)
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Example of a common, but unintuitive expression for the Delta function.
    \item
        Can show that it's true by expressing it as a delta function limit. (If you want to go through it, use Theorem 9.2 from Folland.)
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{What does this mean for Green's functions?}
    
    \begin{align*}
        \L G(x, x') &= \delta(x - x')
    \end{align*}
    actually means
    \begin{align*}
        \left( \L G \right)[\phi]  &= \phi(x') \stackrel{s}{=} \int_{-\infty}^\infty \left( \L G(x, x')\right) \phi(x) \d x
    \end{align*}
\end{frame}

\note{
\begin{itemize}
\item
    Technically, the Green's function is a generalized function such that $ \L G $ is the delta function (it has the sifting property).
\item
    In practise, we'll keep using the less-precise way; just remember that there is a more correct way.
\end{itemize}

}

\begin{frame}[fragile]
    \frametitle{Takeaway}
    
    \begin{center}
        If in doubt, think of $ \delta(x - x_0) $ as an operator, not a function!
    \end{center}

\end{frame}

\note{
\begin{itemize}
\item
    In practise, thinking of $ \delta(x - x_0) $ as a function is usually fine.
    (We'll even do that for the rest of this presentation.)
\item
    But if anything starts to seem fishy, it's good to remember that $ \delta(x - x_0) $ is actually an operator, and not a function.
\end{itemize}
    
}
\fi

\section{Finding the Green's function}
\label{sec:finding_the_green_s_function}
\note{
\begin{itemize}
\item
    In this section, we'll look at one of the simplest methods for actually solving the Green's function problem.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    Original problem:
    \begin{align*}
        \frac{\d^2 u(x)}{\d x^2} - k^2 u(x) &= f(x)
    \end{align*}
    
    Green's function problem:
    \begin{align*}
        \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') &= \delta(x - x')
    \end{align*}
    

\end{frame}

\note{
\begin{itemize}
\item
    Let's start off by looking at a simple example.
\item
    This problem is similar to a simple harmonic oscillator, but the negative sign means we expect lossy behaviour rather than oscillation.
\item
    We won't worry much about boundary conditions yet, we'll just look for solutions that don't blow up at $ x = \pm \infty $.
\item
    If we can find the Green's function, then we can find the solution to the original problem for any $ f(x) $.
\item
    But the Green's function problem looks hard! 
    The point of this example is to demonstrate that we can actually solve it.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    For $ x \neq x' $
    \begin{align*}
        \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') &= 0
    \end{align*}
    
    \pause
    So we have
    \begin{align*}
        G(x,x') &= 
        \begin{cases} 
            A e^{+k (x - x')} & \text{for } x < x'
            \\
            B e^{-k (x - x')} & \text{for } x > x'
        \end{cases}
    \end{align*}
    
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Key thing to notice is that the source is concentrated at $ x = x' $.
    \item
        So for $ x > x' $ and $ x < x' $, we expect the solutions to look like those of the source-free equation.
    \item
        To keep the solutions finite, we expect exponential growth before $ x = x' $ and exponential decay afterward.
    \item
        Now, how do we find the constants $ A $ and $ B $?
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    \begin{align*}
        \frac{\d^2 G(x,x')}{\d x^2} - k^2 G(x,x') &= \delta(x - x')
    \end{align*}

    Continuity of the Green's function:
    \begin{align*}
        \lim_{\epsilon \to 0} \left[ G(x'+\epsilon, x') - G(x' - \epsilon, x') \right] = 0
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    How continuous do we expect our Green's function to be?
\item
    If $ G(x,x') $ is discontinuous (like a step function), then $ \d G / \d x $ will behave like a delta function and $ \d^2 G / \d x^2 $ will behave like a delta function derivative. No good!
\item
    So we expect $ G(x,x') $ to be continuous.
\item
    That gives us one condition we can use to find $ A $ and $ B $. (In fact, it tells us that $ A = B $.)
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}
 
    \begin{align*}
        \alt<2->{\int_{x'-\epsilon}^{x'+\epsilon} \left[ \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') \right] \d x &= \int_{x'-\epsilon}^{x'+\epsilon} \delta(x - x') \d x}{\frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') = \delta(x - x')}
    \end{align*}

    Discontinuity condition:
    \begin{align*}
        \lim_{\epsilon \to 0} \left[ \left. \frac{\d G}{\d x}\right|_{x = x' + \epsilon} - \left. \frac{\d G}{\d x} \right|_{x = x' - \epsilon} \right] &= 1
    \end{align*}
    

\end{frame}

\note{
    \begin{itemize}
    \item
        But what if the derivative $ \d G / \d x $ is discontinuous?
    \item
        Then $ \d^2 G / \d x^2 $ is like a delta function.
        But that's fine, because we have a delta function on the right hand side too.
    \item
        We can find exactly how discontinuous the derivative is by integrating over a small interval around $ x' $.
    \item
        In the limit of $ \epsilon \to 0 $, the second integral vanishes because $ G(x,x') $ is continuous.
    \item
        The first integral is an integral of a derivative, so we can use the fundamental theorem of calculus.
        The result is a \emph{discontinuity condition for the derivative.}
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    \begin{align*}
        G(x,x') &= 
        \begin{cases} 
            A e^{+k (x - x')} & \text{for } x < x'
            \\
            B e^{-k (x - x')} & \text{for } x > x'
        \end{cases}
    \end{align*}
    
    Continuity of $ G(x,x') $:
    \begin{align*}
        A &= B
    \end{align*}

    Discontinuity of $ \dfrac{\d G(x,x')}{\d x} $:
    \begin{align*}
        k A + k B = 1
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Applying our two conditions, we can solve for $ A $ and $ B $.
    We find
    \begin{align*}
        A = B = \frac{1}{2 k}
    \end{align*}
    
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}
    Solving, our Green's function is
    \begin{align*}
        G(x,x') &= \frac{1}{2k} 
        \begin{dcases} 
            e^{+k (x - x')} & \text{for } x < x'
            \\
            e^{-k (x - x')} & \text{for } x > x'
        \end{dcases}
    \end{align*}
    Or, more compactly:
    \begin{empheq}[box=\widefbox]{align*}
        G(x, x') = \frac{e^{k |x - x'|}}{2 k}
    \end{empheq}
    

\end{frame}

\note{
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    Original problem:
    \begin{align*}
        \frac{\d^2 u(x)}{\d x^2} - k^2 u(x) &= f(x)
    \end{align*}

    Solution:
    \begin{align*}
        u(x) &= \int_{-\infty}^{\infty} f(x') \frac{e^{k|x - x'|}}{2k} \d x'
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Now that we have the Green's function, we can construct the solution to our original problem for any forcing function $ f(x) $.
\item
    Caution: remember the fine print from before. 
    This solution only works with certain assumptions about boundary conditions.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{General approach}

    Second-order problems:
    \begin{itemize}
    \item
        $ G(x,x') $ obeys source-free equation for $ x \neq x' $.
    \item
        $ G(x,x') $ is continuous at $ x = x' $.
    \item
        Derivative of $ G(x,x') $ is discontinuous at $ x = x' $.
    \end{itemize}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        This approach works quite well for solving 1D Green's function problems.
    \item
        For problems of other orders, will have a different combination of continuity/discontinuity requirements at $ x = x' $.
        E.g., $ G(x,x') $ will be discontinuous for a first order problem.
    \end{itemize}
}

\section{Constructing the solution}
\label{sec:constructing_the_solution}

\note{
\begin{itemize}
\item
    In the introduction, we showed non-rigorously how to construct a solution from the Green's function.
    To keep things simpler, we ignored boundary conditions.
\item
    Here, we'll look at how to properly construct a solution from the Green's function when boundary conditions are involved.
\item
    Our approach is quite challenging compared to a lot of books on the subject. 
    The advantage is that we'll deal with a lot of subtleties that can otherwise lead to confusion.
\item
    For approaches similar to the one in this section, see Dudley, Morse and Feshbach, or Gerlach.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators}

    \begin{IEEEeqnarray*}{rCrClCrCl}
        \text{Original:} & \qquad & \L [u(x)] &=& f(x); &\qquad& \B [u(x)] &=& 0
        \\
        \text{Adjoint:} & \qquad & \L^* [v(x)] &=& f(x); &\qquad& \B^* [v(x)] &=& 0
    \end{IEEEeqnarray*}
    
    Defining property:
    \begin{align*}
        \inprod{\L u}{v} = \inprod{u}{\L^* v} \qquad \text{where } \inprod{u}{v} = \int_{a}^{b} u(x) v^*(x) \d x
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Underpinning our approach is the concept of an adjoint problem.
    \item
        Suppose we have an original problem defined by operator $ \L $ and boundary conditions $ \B $.
    \item
        Then, $ \L^* $ is the adjoint operator and $ \B^* $ are the adjoint boundary conditions if $ \inprod{\L u}{v} = \inprod{u}{\L^* v} $ for all $ u, v $.
    \item
        Here $ \inprod{u}{v} $ is the inner product as defined on the slide. ($ v^*(x) $ is the complex conjugate of $ v(x) $.)
    \end{itemize}
}

\note{
    Linear algebra notes:
    \begin{itemize}
    \item
        The boundary conditions are important because they specify the domains of $ \L $ and $ \L^* $. (I.e., $ \L $ operates on the Hilbert space of functions $ u(x) $ which satisfy $ \B[u] = 0 $.)
    \item
        So if $ \B \neq \B^* $, then $ \L $ and $ \L^* $ are operators on different Hilbert spaces.
    \item
        If both $ \L = \L^* $ and $ \B = \B^* $, we say that $ \L $ is self-adjoint.
    \item
        If $ \L = \L^* $ but $ \B \neq \B^* $, we say that $ \L $ is \emph{formally} self-adjoint.
    \end{itemize}
}

\note{
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    \begin{align*}
        \L[u(x)] &= \left[ \frac{\d^2}{\d x^2} + k^2 \right] u(x)
        \\
        \B[u(x)] &= \mat{u(a) \\ u(b)} = \mat{0 \\ 0}
    \end{align*}

    Want $ \L^* $ and $ \B^* $ so that
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v}
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Let's look at an example: the 1D simple harmonic oscillator.
    \item
        We'll use boundary conditions so that $ u(a) = u(b) = 0 $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    
    \begin{align*}
        \inprod{\L u}{v} &= \int_a^b \left[ u''(x) + k^2 u(x) \right] v^*(x) \d x
        \\
        \inprod{\L u}{v} &= \int_a^b u(x) \left[ v^{\prime\prime}(x) + (k^2)^* v(x) \right]^* \d x + 
        \\& \quad + \left[ u'(x) v^*(x)  - u(x) v^{\prime *}(x) \right]_{a}^{b}
    \end{align*}
%    We have $ \inprod{\L u}{v} = \inprod{u}{\L v} + [\text{stuff}] $
%    \begin{align*}
%        \L^* v(x) &= \left[ \frac{\d^2}{\d x^2} + (k^2)^* \right] v(x)
%    \end{align*}
    

\end{frame}

\note{
    \begin{itemize}
    \item
        To find the adjoint, let's expand $ \inprod{\L u}{v} $.
    \item
        Use integration by parts twice.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}

    \begin{align*}
        \inprod{\L u}{v} &= \int_a^b u(x) \left[ v^{\prime\prime}(x) + (k^2)^* v(x) \right]^* \d x + 
        \\& \quad + \left[ u'(x) v^*(x)  - u(x) v^{\prime *}(x) \right]_{a}^{b}
    \end{align*}

    If we take
    \begin{align*}
        \L^* &= \frac{\d^2}{\d x^2} + (k^2)^*
    \end{align*}
    then we have
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + \left[ u'(x) v^*(x)  - u(x) v^{*\prime}(x) \right]_{a}^{b}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    The remaining integral term looks like
    \begin{align*}
        \int_a^b u(x) \left[ \L^* v(x) \right]^* \d x 
    \end{align*}
\item
    If we define $ \L^* $ this way, we get closer to our goal.
    We just need to make the last part zero.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + \left[ u'(x) v^*(x)  - u(x) v^{*\prime}(x) \right]_{a}^{b}
    \end{align*}

    Since $ u(a) = u(b) = 0 $,
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + u'(a) v^*(a) - u'(b) v^*(b)
    \end{align*}

    So, pick
    \begin{align*}
        v(a) = v(b) = 0
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        We almost have the required $ \inprod{\L u}{v} = \inprod{u}{\L^* v} $, but we need the part on the right to be zero.
    \item
        From the original problem, we have
        \begin{align*}
            \B[u] &= \mat{u(a) \\ u(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        To make the last part zero, we need the adjoint boundary conditions to be
        \begin{align*}
            \B^*[v] &= \mat{v(a) \\ v(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        So in this case, $ \B = \B^* $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
     
    What if $ u(a) = u'(a) = 0 $ (initial conditions)?
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + \left[ u'(x) v^*(x)  - u(x) v^{*\prime}(x) \right]_{a}^{b}
        \\
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + u'(b) v^*(b)  - u(b) v^{\prime *}(b)
    \end{align*}
    We must have \emph{final} conditions for $ v $:
    \begin{align*}
        v(b) = v'(b) = 0
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        What if we use the same operator $ \L $, but we switch from a boundary value problem to an initial condition problem?
        That is,
        \begin{align*}
            \B[u] &= \mat{u(a) \\ u'(a)} = \mat{0 \\ 0}
        \end{align*}
    \item
        Then, to make the remaining part zero, we need the adjoint boundary conditions to be
        \begin{align*}
            \B[v] &= \mat{v(b) \\ v'(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        For an initial condition problem, the adjoint problem is a \emph{final} condition problem!
        $ \B \neq \B^* $. 
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators}

    In general:
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + J(u,v^*) \Big|_a^b
    \end{align*}
    Definition of adjoint boundary conditions:
    \begin{align*}
        \B[u] = \B^*[v] = 0 \Longleftrightarrow J(u,v^*) \Big|_a^b = 0
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    In the last example, we saw that we had $ \inprod{\L u}{v} $ equal to $ \inprod{u}{\L^* v} $ plus a leftover term which depended on the boundaries.
\item
    This is true more generally: if we don't specify the boundary conditions of $ u $ and $ v $, then we can still \emph{almost} get the adjoint operator equation. We just have a leftover term $ J(u,v^*) |_a^b $, which depends only on the boundary conditions.
    This term is called the conjunct.
\item
    We define adjoint boundary conditions as those boundary conditions which make the conjunct equal to zero.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint Green's functions}
 
    Original problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L [u(x)] &=& f(x); &\qquad& \B [u(x)] &=& \alpha 
    \end{IEEEeqnarray*}
    Green's problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L [G(x,x')] &=& \delta(x - x'); &\qquad& \B [G(x,x')] &=& 0
    \end{IEEEeqnarray*}
    Adjoint Green's problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L^* [H(x,x')] &=& \delta(x - x'); &\qquad& \B^* [H(x,x')] &=& 0
    \end{IEEEeqnarray*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Now we'll be able to deal with boundary conditions properly.
\item
    We define $ G(x,x') $ to obey the same equation as $ u(x) $, but with $ f(x) \to \delta(x - x') $ and $ \alpha \to 0 $.
    As before, $ G(x,x') $ is the impulse response.
\item
    In addition, we define a new function $ H(x,x') $ which is called the adjoint Green's function.
    It obeys the adjoint version of the $ G(x,x') $ equation.
\item
    Warning! A lot of textbooks don't distinguish between $ H(x,x') $ and $ G(x,x') $.
    Sometimes the ``Green's function'' in an expression is really the adjoint Green's function.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Constructing solutions: derivation}

    \begin{align*}
        \inprod{\L u(x)}{H(x,x')} &= \inprod{u(x)}{\L^* H(x,x')} + J\big(u(x), H^*(x,x')\big) \Big|_a^b
        \\
        \inprod{f(x)}{H(x,x')} &= \inprod{u(x)}{\delta(x - x')} + J\big(u(x), H^*(x,x')\big) \Big|_a^b
        \\
        \int_a^b f(x) H^*(x,x') \d x &= \int_a^b u(x) \delta(x - x') \d x + J\big(u(x), H^*(x,x')\big) \Big|_a^b
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x - J\big(u(x), H^*(x,x')\big) \Big|_a^b
    \end{empheq}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        To construct the solution $ u(x) $, we take an inner product of $ \L u(x) $ with $ H(x,x') $, and apply our knowledge of adjoints and conjuncts.
    \item
        Then, we use the fact that $ \L u(x) = f(x) $ and $ \L H(x,x') = \delta(x - x') $.
    \item
        After evaluating the inner product terms, we arrive at a fairly general formula which looks somewhat like what we had in the introduction.
        The difference is that it involves the \emph{adjoint} Green's function $ H(x,x') $, and it has an extra conjunct term.
    \item
        We'll deal with the conjunct later.
        For now, let's try to get rid of $ H(x,x') $ and express $ u(x) $ in terms of $ G(x,x') $.
        To do that, we need a relationship between $ H(x,x') $ and $ G(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Constructing solutions: derivation}

    How are $ G(x,x') $ and $ H(x,x') $ related?
    \begin{align*}
        \inprod{\L G(x,x')}{H(x,x'')} &= \inprod{G(x,x')}{\L^* H(x,x'')}
        \\
        \inprod{\delta(x - x')}{H(x,x'')} &= \inprod{G(x,x')}{\delta(x - x'')}
        \\
        \int_a^b \delta(x - x') H^*(x,x'') \d x &= \int_a^b G(x,x') \delta(x - x'') \d x
        \\
        H^*(x',x'') &= G(x'',x')
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        G(x,x') = H^*(x',x)
    \end{empheq}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Using the definition of the adjoint problem, we find that there is a simple relationship between $ G(x,x') $ and $ H(x,x') $.
    %\item
    %    Note a surprising result of this: if $ G(x,x') $ obeys the boundary conditions with respect to $ x $, then it automatically obeys the \emph{adjoint} boundary conditions with respect to $ x' $. 
    %    This will be important later.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Constructing solutions: derivation}

    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x - J\big(u(x), H^*(x,x')\big) \Big|_a^b
    \end{align*}
    and
    \begin{align*}
        G(x,x') = H^*(x',x)
    \end{align*}
    so
    \begin{empheq}[box=\widefbox]{align*}
        u(x) &= \int_{a}^{b} f(x') G(x,x') \d x' - J\big( u(x'), G(x,x') \big) \Big|_{x'=a}^b
    \end{empheq}
    
\end{frame}

\note{
\begin{itemize}
\item
    Let's go back to our expression for $ u(x') $ in terms of $ H(x,x') $.
\item
    Using our new relationship $ G(x,x') = H^*(x,x') $, we can rewrite this as an expression for $ u(x) $ in terms of $ G(x,x') $.
    (Note: we switched $ x $ and $ x' $ to make it look a little nicer.)
\item
    So this is the more correct version of what we saw in the introduction.
    If the conjunct happens to be zero, then we get what we had before.
    If not, we have an extra term that depends only on the boundaries.
\item
    In general, the conjunct term deals with the boundary conditions of $ u(x) $.
    It turns out that the boundary conditions act, in some way, like additional sources.
    We'll look more closely at this now.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example: 1D Poisson equation}

    Original problem:
    \begin{align*}
        \frac{\d^2 V(x)}{\d x^2} &= - \frac{\rho}{\epsilon_0}; \quad \mat{V(a) \\ V(b)} = \mat{V_a \\ V_b}
    \end{align*}
    
    Green's problem:
    \begin{align*}
        \frac{\d^2 G(x,x')}{\d x^2} &= \delta(x - x'); \quad \mat{G(a,x') \\ G(b,x')} = \mat{0 \\ 0}
    \end{align*}
    
    
\end{frame}

\note{}


\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    Original problem:
    \begin{align*}
        \left( \frac{\d^2}{\d x^2} + k^2 \right) u(x) &= f(x); \quad \mat{u(a) \\ u'(a)} = \mat{\alpha \\ \beta}
    \end{align*}

    Green's problem:
    \begin{align*}
        \left( \frac{\d^2}{\d x^2} + k^2 \right) G(x,x') &= \delta(x - x'); \quad \mat{G(a,x') \\ G'(a,x')} = \mat{0 \\ 0}
    \end{align*}

    Adjoint Green's problem:
    \begin{align*}
        \left( \frac{\d^2}{\d x^2} + k^2 \right) H(x,x') &= \delta(x - x'); \quad \mat{H(b,x') \\ H'(b,x')} = \mat{0 \\ 0}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We'll show how $ H(x,x') $ is useful with an example.
\item
    Here we have a driven simple harmonic oscillator with initial conditions.
    (We'll take $ k $ real for simplicity.)
\item
    From before, we know that the adjoint problem will be the same differential equation, but with final conditions instead of initial conditions.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                A \cos(k(x - x')) + B \sin(k(x - x')) & \text{for } x < x'
                \\
                C \cos(k(x - x')) + D \sin(k(x - x')) & \text{for } x > x'
            \end{cases}
    \end{align*}
    \begin{align*}
        \text{Final conditions } & \Longrightarrow C = D = 0
        \\
        \text{Continuity of function } & \Longrightarrow A = 0
        \\
        \text{Discontinuity of derivative } & \Longrightarrow B = \frac{-1}{k}
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        We can solve for $ H(x,x') $ using a similar approach to before.
    \item
        Except at $ x = x' $, we write $ H(x,x') $ as a solution to the source-free equation.
    \item
        Then we find the coefficients using the boundary conditions and continuity/discontinuity requirements.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                - k^{-1} \sin(k (x - x')) & \text{for } x < x'
                \\
                0 & \text{for } x > x'
            \end{cases} 
    \end{align*}
 
\end{frame}

\note{
    \begin{itemize}
    \item
        So we have our adjoint Green's function.
    \item
        Now we just need to figure out how to construct $ u(x) $ from it.
    \end{itemize}
}
\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
    Expand $ J(u,H) $:
    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x - \left[ \frac{\d u(x)}{\d x} H^*(x,x') - u(x) \frac{\d H^*(x,x')}{\d x} \right]_a^b
    \end{align*}

    Recall:
    \begin{align*}
        \B[u] = \mat{u(a) \\ u'(a)} = \mat{\alpha \\ \beta}; \qquad \B^*[H] = \mat{H(b,x') \\ H'(b,x')} = \mat{0 \\ 0}
    \end{align*}
    
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Now let's expand $ J(u,v) $ for our particular example. (We can basically copy it from a previous part of the derivation.)
    \item
        We can simplify the conjunct by remembering our boundary conditions.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x + \beta H^*(a,x') - \alpha \frac{\d H^*(a,x')}{\d x}
    \end{align*}
   
\end{frame}

\note{
    \begin{itemize}
    \item
        We're very close to our final goal. 
    \item
        The last thing will be to get rid of $ H(x,x') $ and replace it with $ G(x,x') $.
    \end{itemize}
    
}
\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
     
    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                - k^{-1} \sin(k (x - x')) & \text{for } x < x'
                \\
                0 & \text{for } x > x'
            \end{cases} 
    \end{align*}

    \begin{align*}
        G(x,x') &= 
            \begin{cases} 
                0 & \text{for } x < x'
                \\
                k^{-1} \sin(k (x - x')) & \text{for } x > x'
            \end{cases} 
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Since we already know the adjoint Green's function $ H(x,x') $, we can use find the Green's function via $ G(x,x') = H^*(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
 
    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x + \beta H^*(a,x') - \alpha \left. \frac{\d H^*(x,x')}{\d x} \right|_{x=a}
        \pause
        \\
        u(x') &= \int_a^b f(x) G(x',x) \d x + \beta G(x',a) - \alpha \left. \frac{\d G(x',x)}{\d x} \right|_{x=a}
        \pause
        \\
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        With our new knowledge that $ G(x,x') = H^*(x',x) $, we can rewrite the solution $ u(x) $ in terms of $ G(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Final solution}

    \begin{empheq}[box=\widefbox]{align*}
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
    \end{empheq}
    where
    \begin{align*}
        G(x,x') &= 
            \begin{cases} 
                0 & \text{for } x < x'
                \\
                k^{-1} \sin(k (x - x')) & \text{for } x > x'
            \end{cases} 
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Finally, we arrive at our solution.
\item
    First, note that the final answer does \emph{not} depend on $ H(x,x') $. We didn't actually need to ever calculate $ H(x,x') $ from the adjoint Green's equation, we could have just found $ G(x,x') $ from the (non-adjoint) Green's equation.
\item
    However, it would have been very difficult to derive this expression without using $ H(x,x') $ (I couldn't see an easy way).
    Because of this, a lot of authors stop at the expression for $ u(x') $ in terms of $ H^*(x,x') $, and they just call $ H^*(x,x') $ the ``Green's function.''
\item
    By doing the extra work, though, we gain a very nice interpretation for the boundary conditions term.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Interpretation of boundary conditions}
    
    \begin{align*}
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
        \\
        u(x) &= \int_a^b \Big[ f(x') + \beta \delta(x' - a) - \alpha \delta'(x' - a) \Big] G(x,x') \d x'
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We note that our expression for $ u(x) $ looks like it did before (integral over $ f(x') G(x,x') $), but now there are extra terms which depend on the boundary conditions.
\item
    We come to a key idea: boundary conditions have the same effect on $ u(x) $ as adding little impulse sources at the boundary.
    The Green's function can deal with both sources $ f(x) $ and non-zero boundary conditions.
\item
    Be careful, though: $ G(x,x') $ still depends on the \emph{type} of boundary condition.
    E.g., we use the same $ G(x,x') $ for all initial value problems ($ u(a), u'(a) $ specified), but we'll need a different $ G(x,x') $ for boundary value problems ($ u(a), u(b) $ specified).
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Summary}
    \vspace{-24pt}
    \begin{IEEEeqnarray*}{rClCrCl}
        \L[u(x)] &=& f(x); &\qquad& \B[u(x)] &=& \alpha
        \\
        \L[G(x,x')] &=& \delta(x - x'); &\qquad& \B[G(x,x')] &=& 0
        \\
        \L^*[H(x,x')] &=& \delta(x - x'); &\qquad& \B^*[H(x,x')] &=& 0
    \end{IEEEeqnarray*}

    \begin{enumerate}
    \item
        Find $ G(x,x') $.
    \item
        Find $ u(x') $ in terms of $ H(x,x') $:
        \begin{align*}
            u(x') &= \int_a^b f(x) H^*(x,x') \d x - J(u(x), H(x,x'))\Big|_a^b
        \end{align*}
    \item
        Express $ u(x) $ in terms of $ G(x,x')= H^*(x',x) $.
    \end{enumerate}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        That was a long process, so let's summarize what we did.
    \item
        First, we wrote out the original equation, the Green's function equation, and the adjoint Green's function equation. (Take note of the boundary conditions in particular.)
    \item
        Next, we solve the Green's function equation for $ G(x,x') $.
    \item
        Next, we found that it's much easier to express $ u(x) $ in terms of $ H(x,x') $, because we can use inner products.
        The only tricky part is finding the conjunct. 
        (Usually, just requires integration by parts.
        See Dudley for a general formula for Sturm-Liouville problems.)
    \item
        Finally, we replace $ H(x,x') $ with $ G^*(x',x) $ (which we solved for previously), and we have our final expression for $ u(x) $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Summary}

    \begin{IEEEeqnarray*}{rClCrCl}
        \L[u(x)] &=& f(x); &\qquad& \B[u(x)] &=& \alpha
        \\
        \L[G(x,x')] &=& \delta(x - x'); &\qquad& \B[G(x,x')] &=& 0
    \end{IEEEeqnarray*}

    \begin{empheq}[box=\widefbox]{align*}
        u(x) &= \int_{a}^{b} f(x') G(x,x') \d x' - J\big( u(x'), G(x,x') \big) \Big|_{x'=a}^b
    \end{empheq}
    Simplify with adjoint boundary conditions!
    
\end{frame}

\note{
\begin{itemize}
\item
    We can streamline this process a little bit by removing the intermediate steps involving $ H(x,x') $.
\item
    Then, we can simply write down our final solution in terms of the Green's function $ G(x,x') $.
\item
    However, there are still traces of the adjoint problem.
\item
    First, we cannot find the conjunct $ J(u,v) $ without considering the adjoint problem.
\item
    Second, we have to simplify the expression $ J(u,G)|_a^b $ using the adjoint boundary conditions.
    In particular, use the fact that $ G(x,x') $ obeys the adjoint boundary conditions with respect to $ x' $.
\end{itemize}
}

\section{Properties of Green's functions}
\label{sec:properties_of_green_s_functions}

\note{
    \begin{itemize}
    \item
        The Green's function gives us a lot of information about the system we're dealing with.
    \item
        Here we'll look at a few of the properties Green's functions can have and what those tell us about our system.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Reciprocity}

    If $ \L $ is self-adjoint
    \begin{align*}
        \L = \L^* \qquad \text{and} \qquad \B = \B^*
    \end{align*}
    then
    \begin{align*}
        G(x,x') &= G^*(x',x)
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        For self-adjoint problems, $ G = H $. 
    \item
        Using our relationship between $ G $ and $ H $, we quickly see that the Green's function is complex symmetric (Hermitian) for self-adjoint problems.
    \item
        Roughly, putting a source at $ x $ and measuring at $ x' $ is the same as putting a source at $ x' $ and measuring at $ x $.
        This is often called reciprocity (though it's not quite the same as reciprocity from time-harmonic E\&M, because we're using a true inner product rather than the so-called ``reaction inner product'').
    \item
        Note that we require \emph{both} $ \L = \L^* $ and $ \B = \B^* $. So this would likely not hold for initial condition problems.
    \end{itemize}
}


\begin{frame}[fragile]
    \frametitle{Invariance}


    $ \L $ is invariant if 
    \begin{align*}
        \L[u(x - \xi)] &= \L[u(x)] \Big|_{x = x - \xi}
    \end{align*}
    
    For example,
    \begin{IEEEeqnarray*}{rClCl}
        \L &=& a \frac{\d^2}{\d x^2} + b \frac{\d}{\d x} + c
    \end{IEEEeqnarray*}
    is invariant only if $ a $, $ b $, and $ c $ are constants.
    
    
\end{frame}

\note{
\begin{itemize}
\item
    An operator is invariant if shifting the input just results in a shifted output.
\item
    Probably familiar from signal processing: linear \emph{time-invariant} systems.
    Delaying the input signal leads to the same output signal, just delayed by the same amount.
\item
    Invariance is really important in modern physics.
\item
    Maxwell's equations in free space are invariant with respect to $ x, y, z, \phi, \theta $.
    That is, at a fundamental level, the laws of electromagnetism do not change if we move to a different location or look in a different direction.
\end{itemize}

}

\begin{frame}[fragile]
    \frametitle{Invariance}

    If $ \L $ is invariant in $ x $, then
    \begin{align*}
        \L[G(x,x')] &= \delta(x - x')
        \\
        \L[G(x - \xi,x' - \xi)] &= \delta(x - x')
    \end{align*}
    \begin{align*}
        \Longrightarrow G(x,x') = G(x - \xi, x' - \xi)
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        G(x, x') &= G(x - x')
    \end{empheq}
    
\end{frame}

\note{
\begin{itemize}
\item
    For invariant problems, we can see that shifting both $ x $ and $ x' $ by the same amount $ \xi $ does not affect the Green's function.
\item
    Taking $ \xi  = x' $, we see that $ G(x,x') $ is actually only a function of the difference $ (x - x') $.
\item
    That is, the response only depends on the \emph{relative} locations of the source and measurement.
    This fits nicely with our intuitive understanding of invariance.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Invariance}
    
    Convolution:
    \begin{align*}
        u(x) &= \int_a^b G(x - x') f(x') \d x' = G(x) * f(x)
    \end{align*}
    Frequency domain:
    \begin{align*}
        \ft{u}(k) &= \ft{G}(k) \ft{f}(k)
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    For invariant systems (with boundary conditions zero), the solution $ u(x) $ is just given as a convolution of the source function $ f(x) $ with the impulse response $ G(x) $.
\item
    Taking Fourier transforms, the convolution turns into multiplication. 
\item
    Looks familiar from signal processing!
\end{itemize}
}


\section{Spectral methods}
\label{sec:spectral_methods}

\note{
\begin{itemize}
\item
    In this section, we'll look at the strong relationship between Green's functions and spectral theory.
\item
    Essentially, eigenfunction expansion allows us to calculate the Green's function when direct methods don't work.
\item
    A basic background in spectral theory can be found in most books covering Green's functions. 
\item
    Unfortunately, these are rarely rigorous when dealing with continuous sets of eigenvalues.
    For fully rigorous spectral theory (not for the faint of heart!), see Naylor and Sell's \emph{Linear operator theory in engineering and science} or Kreyszig's \emph{Introductory functional analysis with applications.}
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}

    Problem:
    \begin{align*}
        \left( \L - \lambda \right)u(x) &= f(x); \qquad \B[u(x)] = 0
    \end{align*}
    where $ \L $ is self-adjoint:
    \begin{align*}
        \L = \L^* \quad \text{and} \quad \B = \B^*
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        First we'll review a bit of eigenfunction theory, but we'll quickly see how it relates to Green's functions.
    \item
        Set up a similar problem as before, but we've added a complex parameter $ \lambda $ for later convenience.
    \item
        Also, for this section we'll insist that $ \L $ be fully self-adjoint so that we can take full advantage of spectral theory.
    \item
        A brief discussion of the non-self-adjoint case can be found in Morse and Feshbach under ``Non-Hermitian operators: biorthogonal functions''.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}
    
    Eigenfunctions:
    \begin{align*}
        \L \phi_n(x) &= \lambda_n \phi_n(x)
    \end{align*}
    
    With $ \L $ self-adjoint, $ \lambda_n \in \mathbb{R} $ and
    \begin{align*}
        u(x) &= \sum_n \inprod{u}{\phi_n} \phi_n(x)
        \\
        f(x) &= \sum_n \inprod{f}{\phi_n} \phi_n(x)
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Because $ \L $ is self-adjoint, we know that its eigenvalues $ \lambda_n $ are real.
\item
    We also know that it has a complete orthonormal set of eigenfunctions $ \phi_n $.
\item
    That is, we can expand any function (in this case $ u(x) $ and $ f(x) $) in terms of $ \phi_n(x) $. (Generalized Fourier series.)
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}

    \begin{align*}
        (\L - \lambda)u(x) &= f(x)
        \\
        (\L - \lambda) \left[ \sum_n \inprod{u}{\phi_n} \phi_n(x) \right] &= \sum_n \inprod{f}{\phi_n} \phi_n(x)
        \\
        \sum_n \inprod{u}{\phi_n} (\lambda_n - \lambda) \phi_n(x) &= \sum_n \inprod{f}{\phi_n} \phi_n(x)
        \\
        (\lambda_n - \lambda) \inprod{u}{\phi_n} &= \inprod{f}{\phi_n}
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Going back to our original equation, let's expand $ u(x) $ and $ f(x) $ in terms of eigenfunctions of $ \L $.
    \item
        Using the fact that $ \L $ is linear and $ \L \phi_n = \lambda_n \phi_n $, we can get rid of $ \L $ (third line).
    \item
        Finally, since the $ \phi_n(x) $ are linearly independent, each term in the sums on the RHS and LHS must be equal.
        So we get an expression for the generalized Fourier coefficients $ \inprod{u}{\phi_n} $.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}

    \begin{align*}
        \inprod{u}{\phi_n} &= \frac{\inprod{f}{\phi_n}}{\lambda_n - \lambda}
    \end{align*}

    So
    \begin{align*}
        u(x) &= \sum_n \inprod{u}{\phi_n} \phi_n(x)
        \\
        \Aboxed{u(x) &= \sum_n \frac{\inprod{f}{\phi_n}}{\lambda_n - \lambda} \phi_n(x)}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Plugging in our new expression for the Fourier coefficients, we obtain a formula for $ u(x) $ in terms of the eigenfunctions and eigenvalues of $ \L $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}

    \vspace{-16pt}
    \begin{align*}
        u(x) &= \sum_n \frac{\inprod{f}{\phi_n}}{\lambda_n - \lambda} \phi_n(x)
        \\
        u(x) &= \sum_n \left( \int_a^b \frac{f(x') \phi^*_n(x')}{\lambda_n - \lambda} \d x' \right) \phi_n(x)
        \\
        u(x) &= \int_a^b \left( \sum_n \frac{f(x) \phi^*_n(x')}{\lambda_n - \lambda} \phi_n(x) \right) \d x'
        \\
        u(x) &= \int_a^b \left( \alert{\sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda} } \right) f(x') \d x'
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Usually, the inner product is defined by an integral.
\item
    If we write this out and do some manipulation, we get something that looks a lot like the Green's function expression.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Spectral form of the Green's function}
    
    \begin{align*}
        u(x) &= \int_a^b \left( \sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda} \right) f(x') \d x'
    \end{align*}

    \begin{empheq}[box=\widefbox]{align*}
        G(x,x') &= \sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda}
    \end{empheq}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        It turns out that we actually can read off this weird sum as the Green's function.
    \item
        So, if we know the eigenvalues and eigenfunctions of $ \L $, we can immediately construct the Green's function as an infinite series.
    \item
        Note also that $ G(x,x') = G^*(x,x') $, as we expect because this is a self-adjoint problem.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Spectral form of the Green's function}

    Green's function of $ (\L - \lambda) $:
    \begin{align*}
        G(x,x',\lambda) &= \sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda}
    \end{align*}

    $ \lambda_n $ are poles of $ G(x,x',\lambda) $.

    $ \phi_n(x) $ can be found by residue integration.
    
\end{frame}

\note{
\begin{itemize}
\item
    It also goes the other way. If we know the Green's function of $ (\L - \lambda) $ for any complex $ \lambda $, then the eigenvalues of $ \L $ are just the poles of the Green's function with respect to lambda.
\item
    Eigenfunctions are a little trickier to read off, but it's possible to find them from the Green's function using residue integration.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Spectral form of the delta function}

    \begin{align*}
        \delta(x - x') &= (\L - \lambda) G(x,x') 
        \pause
        \\
        \delta(x - x') &= (\L - \lambda) \sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda}
        \pause
        \\
        \delta(x - x') &= \sum_n \frac{(\lambda_n - \lambda) \phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda}
        \pause
        \\
        \Aboxed{\delta(x - x') &= \sum_n \phi_n(x) \phi^*_n(x')}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Using our Green's function equation, we can also derive an expression for the delta function as a sum of eigenfunctions.
\item
    This expression is useful when solving three-dimensional problems with separation of variables.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example: simple harmonic oscillator}

    \begin{align*}
        \underbrace{\left( \frac{\d^2}{\d x^2} - \lambda \right)}_{\displaystyle \L - \lambda} u(x) = f(x); \quad u(0) = u(a) = 0
    \end{align*}

    Eigenfunctions of $ \L $:
    \begin{align*}
        \phi_n(x) &= \sqrt{\frac{2}{a}} \sin\left( \frac{\pi n x}{a} \right); \quad \lambda_n = \frac{\pi n}{a}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Let's do a simple example to illustrate the idea.
\item
    For $ \L = \d^2/\d x^2 $ we know that the eigenfunctions are sines and cosines.
    The boundary conditions restrict us to just sines with $ \lambda_n = \pi n / a $.
\item
    $ \sqrt{2/a} $ ensures that the eigenfunctions are normalized.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example: simple harmonic oscillator}
    
    \begin{align*}
        G(x,x') &= \sum_n \frac{\phi_n(x) \phi_n^*(x')}{\lambda_n - \lambda}
        \\
        G(x,x') &= \sum_{n=0}^{\infty} \frac{2 \sin\Big( \dfrac{\pi n x}{a} \Big)\sin\Big( \dfrac{\pi n x'}{a} \Big)}{\pi n - \lambda a}
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Using the formula we derived earlier, we can very quickly write out the Green's function as an infinite series.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example: simple harmonic oscillator}

    \begin{align*}
        G(x,x') &= \sum_{n=0}^{\infty} \frac{2 \sin\Big( \dfrac{\pi n x}{a} \Big)\sin\Big( \dfrac{\pi n x'}{a} \Big)}{\pi n - \lambda a}
    \end{align*}
 
    Compare with direct method:
    \begin{align*}
    G(x,x') &= \begin{cases} \frac{\sin\left(\sqrt{\lambda}(a - x')\right) \sin\left(\sqrt{\lambda} x\right)}{\sqrt{\lambda} \sin\left(\sqrt{\lambda} a\right)} & \text{for } x < x' \\ \frac{\sin\left(\sqrt{\lambda} x'\right) \sin\left(\sqrt{\lambda}(a - x)\right)}{\sqrt{\lambda} \sin\left(\sqrt{\lambda} a\right)} & \text{for } x > x' \end{cases}
    \end{align*}
       
\end{frame}

\note{
\begin{itemize}
\item
    We could have also solved this problem directly (Assume $ G(x,x') $ behaves like the source-free solution except at $ x = x' $. Apply the boundary conditions, continuity and discontinuity requirements to find the coefficient.) The result is shown.
\item
    The direct solution is a little uglier, but it's much easier to evaluate numerically because it doesn't involve an infinite series.
    For that reason, direct solution is usually more desireable if it actually works. 
    However, series solutions tend to be needed for solving multi-dimensional problems.
\item
    Note: there's a trick for evaluating infinite series using residue calculus, and (I think) you could use this to derive the second expression from the first. You can also use residue integration to derive the first from the second.
\end{itemize}

}

\ifextended
\section{The 3D wave equation}
\label{sec:3d_wave_equation}

\note{
\begin{itemize}
\item
    Unfortunately we don't have time to really get into 3D problems.
\item
    However, just to get a taste of it, we'll look at the 3D scalar wave equation in a vacuum.
\item
    I chose time-domain because it's something that's less-frequently covered in electrical engineering books.
\item
    The time domain also gives insight to some delicate issues in the frequency domain.
\end{itemize}

}

\begin{frame}[fragile]
    \frametitle{The wave equation problem}

    \begin{align*}
        \underbrace{\left( \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2} \right)}_{\displaystyle \L} u(\vect{r}, t) &= f(\vect{r}, t); \qquad \B[u] = \alpha
    \end{align*}

    In electromagnetism:
    \begin{align*}
        \left( \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2} \right) \vect{A}(\vect{r}, t) &= - \vect{J}(\vect{r}, t)
    \end{align*}
    

\end{frame}

\note{
\begin{itemize}
\item
    The wave equation is a key component of electromagnetism. 
    Most of this course was spent learning different ways to solve the Helmholtz equation, which is just a Fourier transformed version of the wave equation.
\item
    In the time domain (in the Lorentz gauge), each component of the vector potential $ \vect{A} $ obeys the scalar wave equation, with a component of $ \vect{J} $ as its source.
\item
    We can solve a huge number of electromagnetics problems if we can solve the scalar wave equation (or the scalar Helmholtz equation).
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{The adjoint wave equation}

    Find $ \L $, $ \L^* $, $ \B $ and $ \B^* $ so that
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v}
    \end{align*}
    where
    \begin{align*}
        \inprod{u}{v} = \int_{t_i}^{t_f} \int_V u(\vect{r}, t) v^*(\vect{r}, t) \d^3 \vect{r} \d t
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    To construct $ u(\vect{r},t) $ in terms of the Green's function, we proceed similar to the 1D case.
\item
    The first step is to find the adjoint operators and adjoint boundary conditions for the wave equation.
\item
    Before our inner product was an integral over the single variable $ x $.
    Now, it's an integral over all four variables $ x, y, z, t $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{The adjoint wave equation}

    \vspace{-24pt}
    \begin{align*}
        \inprod{\L u}{v} &= \int_{t_i}^{t_f} \int_V \left( \nabla^2 u - \frac{1}{c^2} \frac{\partial^2 u}{\partial t^2} \right) v^* \d^3 \vect{r} \d t
    \end{align*}
    
    Use Green's identity
    \begin{align*}
        \int_V \left( v^* \nabla^2 u - u \nabla^2 v^* \right) &= \oint_{\partial V} \left( v^* \frac{\partial u}{\partial n} - u \frac{\partial v^*}{\partial n} \right) \d S
    \end{align*}
    and integration by parts
    \begin{align*}
        \int_{t_i}^{t_f} \left( \frac{\partial^2 u}{\partial t^2} v^* - u \frac{\partial^2 v^*}{\partial t^2} \right) \d t &= \left[ v^* \frac{\partial u}{\partial t} - u \frac{\partial v^*}{\partial t} \right]_{t_i}^{t_f}
    \end{align*}
\end{frame}

\note{
\begin{itemize}
\item
    To find $ \L^* $ and $ \B^* $, we write out $ \inprod{\L u}{v} $ and then try to rewrite it in the form $ \inprod{u}{\L^* v} $ using theorems from calculus.
\item
    Green's identity is essentially a 3D version of the integration by parts that we used in the 1D case.
    Note that $ V $ is a volume and $ \partial V $ is the boundary of that volume.
\item
    We use Green's identity to deal with the spatial derivative part $ v^* \nabla^2 u $.
\item
    We use 1D integration by parts to deal with the time derivative part.
\end{itemize}

}

\begin{frame}[fragile]
    \frametitle{The adjoint wave equation}

    Result:
    \begin{IEEEeqnarray*}{rCl}
        \int_{t_i}^{t_f} \int_V \left( \L u \right) v^* \d^3 \vect{r} \d t &=& \int_{t_i}^{t_f} \int_V u \left( \L v \right)^* \d^3 \vect{r} \d t + 
        \\ && + \int_{t_i}^{t_f} \oint_{\partial V} \left( v^* \frac{\partial u}{\partial n} - u \frac{\partial v^*}{\partial n} \right) \d S \d t - 
        \\ && - \frac{1}{c^2} \left[ \int_{V} \left( v^* \frac{\partial u}{\partial t} - u \frac{\partial v^*}{\partial t} \right) \d V \right]_{t_i}^{t_f}
    \end{IEEEeqnarray*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Not pretty! Important thing is that it's similar to what we had in the 1D case.
\item
    The first term is $ \inprod{\L u}{v} $ and the second term is $ \inprod{u}{\L v} $.
    So, for the wave equation $ \L = \L^* $.
\item
    The last two terms are just a big ugly conjunct.
    The first one depends only on the spatial boundary conditions ($ \partial V $ is the boundary of the volume $ V $), while the second one depends only on the temporal boundary conditions.
\item
    Given boundary conditions on $ u $, the adjoint boundary conditions are the ones that make these last two terms go to zero.
    E.g., if $ u(\vect{r}, t) = 0 $ over the whole boundary, then we would need $ v(\vect{r}, t) = 0 $ over the whole boundary as well to cancel out the second-last integral.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Green's problems}

    Original problem:
    \begin{align*}
        \L u(\vect{r}, t) &= f(\vect{r}, t); \quad \B[u] = \alpha
    \end{align*}
    Green's problem:
    \begin{align*}
        \L G(\vect{r}, t; \vect{r}', t') &= \delta^3(\vect{r} - \vect{r}') \delta(t - t'); \quad \B[G] = 0
    \end{align*}
    Adjoint Green's problem:
    \begin{align*}
        \L H(\vect{r}, t; \vect{r}', t') &= \delta^3(\vect{r} - \vect{r}') \delta(t - t'); \quad \B^*[H] = 0
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    As in the 1D case, we set up three problems.
\item
    For the lossless wave equation $ \L = \L^* $, so the only difference between the $ G $ and $ H $ equations is the boundary conditions.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Solution to the wave equation}

    \vspace{-24pt}
    \begin{IEEEeqnarray*}{rCl}
        u(\vect{r}', t') &=& \int_{t_i}^{t_f} \int_V f(\vect{r}, t) H^*(\vect{r}, t; \vect{r}', t') \d^3 \vect{r} \d t + 
        \\ && - \int_{t_i}^{t_f} \oint_{\partial V} \left( H^* \frac{\partial u}{\partial n} - u \frac{\partial H^*}{\partial n} \right) \d S \d t - 
        \\ && + \frac{1}{c^2} \left[ \int_{V} \left( H^* \frac{\partial u}{\partial t} - u \frac{\partial H^*}{\partial t} \right) \d V \right]_{t_i}^{t_f}
    \end{IEEEeqnarray*}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Solution to the wave equation}

    This time, $ G(\vect{r}, t; \vect{r}', t') = H^*(\vect{r}', t'; \vect{r}, t) $ so
    \begin{IEEEeqnarray*}{rCl}
        u(\vect{r}, t) &=& \int_{t_i}^{t_f} \int_{V'} f(\vect{r}', t') G(\vect{r}, t; \vect{r}', t') \d^3 \vect{r}' \d t' + 
        \\ && - \int_{t_i}^{t_f} \oint_{\partial V'} \left( G \frac{\partial u}{\partial n'} - u \frac{\partial G}{\partial n'} \right) \d S' \d t' - 
        \\ && + \frac{1}{c^2} \left[ \int_{V'} \left( G \frac{\partial u}{\partial t'} - u \frac{\partial G}{\partial t'} \right) \d V' \right]_{t_i}^{t_f}
    \end{IEEEeqnarray*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Not very pretty, but we have a full solution for $ u(\vect{r}, t) $ in terms of the Green's function.
\item
    The first integral gives the effect of the source term $ f(\vect{r}, t) $.
\item
    The second integral gives the effect of the boundary conditions on $ u $.
\item
    The third integral gives the effect of the initial conditions on $ u $.
\item
    Note that $ G(\vect{r}, t; \vect{r}', t') $ obeys the boundary conditions with respect to $ \vect{r}, t $ and the \emph{adjoint} boundary conditions with respect to $ \vect{r}', t' $.
    Use this for a given problem to simplify the last two terms.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Finding the Green's function}

    \begin{align*}
        \left( \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2} \right) G(\vect{r}, t; \vect{r}', t') &= \delta^3(\vect{r} - \vect{r}') \delta(t - t')
    \end{align*}

    Initial conditions:
    \begin{align*}
        G(\vect{r}, 0; \vect{r}', t') &= \left. \frac{\partial G(\vect{r},t; \vect{r}', t')}{\partial t} \right|_{t = 0} = 0
    \end{align*}
    
    Boundary conditions:
    \begin{align*}
        \lim_{|\vect{r}| \to \infty} G(\vect{r}, t; \vect{r}', t') &\to 0
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Now let's look at how we can actually solve the Green's function problem.
\item
    We'll assume that we have an initial condition problem, and that we are interested in all of space (boundaries are at infinity).
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Finding the Green's function}

    Translation invariance:
    \begin{align*}
        G(\vect{r}, t; \vect{r}', t') &= G(\vect{r} - \vect{r}', t - t')
    \end{align*}

    Let $ \vect{R} = \vect{r} - \vect{r}' $ and $ \tau = c(t - t') $ so that 
    \begin{align*}
        \left( \nabla_R^2 - \frac{\partial^2}{\partial \tau^2} \right) G(\vect{R}, \tau) &= \delta^3(\vect{R}) \delta(\tau)
    \end{align*}

   
\end{frame}

\note{
\begin{itemize}
\item
    Because the wave equation is translation invariant, we can simplify the Green's function to make the problem easier to solve.
\item
    We define new variables $ \vect{R} = \vect{r} - \vect{r}' $ and $ \tau = t - t' $.
\item
    Note that $ \nabla^2 $ is the laplacian with respect to $ \vect{R} $.
\end{itemize}

}

\begin{frame}[fragile]
    \frametitle{Finding the Green's function}
    
    \begin{align*}
        \left( \nabla_R^2 - \frac{\partial^2}{\partial \tau^2} \right) G(\vect{R}, \tau) &= \delta^3(\vect{R}) \delta(\tau)
    \end{align*}

    Spatial Fourier transform:
    \begin{align*}
        \left( k^2 - \frac{\partial^2}{\partial \tau^2} \right) \ft{G}(\vect{k}, \tau) &= \delta(\tau)
    \end{align*}
    so
    \begin{align*}
        \ft{G}(\vect{k},\tau) &= 
        \begin{cases}
            A \cos(k \tau) + B\sin(k \tau) & \text{for } \tau < 0
            \\
            C \cos(k \tau) + D\sin(k \tau) & \text{for } \tau > 0
        \end{cases}
    \end{align*}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Finding the Green's function}

    
    \vspace{-16pt}
    \begin{align*}
        \ft{G}(\vect{k},\tau) &= 
        \begin{cases}
            A \cos(k \tau) + B\sin(k \tau) & \text{for } \tau < 0
            \\
            C \cos(k \tau) + D\sin(k \tau) & \text{for } \tau > 0
        \end{cases}
    \end{align*}
    
    \begin{itemize}
    \item
        Initial conditions give $ A = B = 0 $.
    \item
        Continuity at $ t = t' $ gives $ C = 0 $.
    \item
        Discontinuity at $ t = t' $ gives $ D = -1/k $.
    \end{itemize}

    \begin{align*}
        \ft{G}(\vect{k},\tau) &= %- \frac{\sin(k \tau)}{k} U(\tau)
        \begin{cases}
            0 & \text{for } \tau < 0
            \\
            -\dfrac{\sin(k \tau)}{k} & \text{for } \tau > 0
        \end{cases}
    \end{align*}

\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Finding the Green's function}

    Inverse Fourier transform for $ \tau > 0 $:
    \begin{align*}
        G(\vect{R},\tau) &= \frac{1}{8 \pi^3} \int_{\mathbb{R}^3} \ft{G}(\vect{k},\tau) e^{j \vect{k} \cdot \vect{R}} \d^3 \vect{k}
        \\
        G(\vect{R},\tau) &= \frac{-1}{8 \pi^3} \int_{\mathbb{R}^3} \frac{\sin(k \tau)}{k} e^{j \vect{k} \cdot \vect{R}} \d^3 \vect{k}
    \end{align*}

    Use spherical coordinates in $ \vect{k} $: $ (k, \theta, \phi) $.
    \begin{align*}
        G(\vect{R},\tau) &= \frac{-1}{8 \pi^3} \int_0^{2 \pi} \int_0^\pi \int_0^\infty \frac{\sin(k \tau)}{k} e^{j k R \cos(\theta)} k^2 \sin(\theta) \d k \d \theta \d \phi
    \end{align*}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Finding the Green's function}
    
    \begin{align*}
        G(\vect{R},\tau) &= \frac{-1}{8 \pi^3} \int_0^{2 \pi} \int_0^\pi \int_0^\infty \frac{\sin(k \tau)}{k} e^{j k R \cos(\theta)} k^2 \sin(\theta) \d k \d \theta \d \phi
        \\
        G(\vect{R},\tau) &= \frac{-1}{4 \pi^2} \int_0^\infty k \sin(k \tau) \left[ \int_0^\pi \sin(\theta) e^{j k R \cos(\theta)} \d \theta \right] \d k
        \\
        G(\vect{R},\tau) &= \frac{-1}{4 \pi^2} \int_0^\infty k \sin(k \tau) \left[ \frac{2 \sin(k R)}{k R} \right] \d k
        \\
        G(\vect{R},\tau) &= \frac{-1}{2 \pi^2 R} \int_0^\infty \sin(k \tau) \sin(k R) \d k
    \end{align*}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Finding the Green's function}

    \begin{align*}
        G(\vect{R},\tau) &= \frac{-1}{2 \pi^2 R} \int_0^\infty \sin(k \tau) \sin(k R) \d k
        \\
        G(\vect{R},\tau) &= \frac{-1}{4 \pi^2 R} \int_{-\infty}^\infty \sin(k \tau) \sin(k R) \d k
        \\
        G(\vect{R},\tau) &= \frac{-1}{4 \pi^2 R} \int_{-\infty}^\infty \left( \frac{e^{j k \tau} - e^{-j k \tau}}{2 j}\right) \left( \frac{e^{j k R} - e^{-j k R}}{2j}\right) \d k
        \\
        G(\vect{R},\tau) &= \frac{1}{16 \pi^2 R} \int_{-\infty}^\infty \left( e^{j k (\tau + R)} - e^{-j k (\tau - R)} - e^{j k (\tau - R)} + e^{-j k (\tau + R)} \right) \d k
    \end{align*}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Finding the Green's function}
    
    \begin{align*}
        G(\vect{R},\tau) &= \frac{1}{16 \pi^2 R} \int_{-\infty}^\infty \left( e^{j k (\tau + R)} - e^{-j k (\tau - R)} - e^{j k (\tau - R)} + e^{-j k (\tau + R)} \right) \d k
        \\
        G(\vect{R},\tau) &= \frac{1}{16 \pi^2 R} 2 \pi \left[ \delta(\tau + R) - \delta(\tau - R) - \delta(\tau - R) + \delta(\tau + R) \right]
    \end{align*}
    But $ \tau > 0 $ and $ R > 0 $ so $ \delta(\tau + R) = 0 $ and we have
    \begin{align*}
        G(\vect{R},\tau) &= \frac{- \delta(\tau - R)}{4 \pi R}
    \end{align*}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Green's function for the wave equation}
    
    \begin{align*}
    G(\vect{r}, t; \vect{r}', t') &= \begin{cases} - \frac{\delta\left( c(t - t') - \left| \vect{r} - \vect{r'} \right| \right)}{4 \pi \left| \vect{r} - \vect{r}' \right|} & \text{for } t > t' \\ 0 & \text{for } t < t' \end{cases}
    \end{align*}

    With zero boundary/initial conditions:
    \begin{align*}
        u(\vect{r}, t) &= \int_{t_i}^{t_f} \int_{\mathbb{R}^3} G(\vect{r}, t; \vect{r}', t') f(\vect{r}', t') \d^3 \vect{r}' \d t'
        \\
        u(\vect{r}, t) &= \int_{\mathbb{R}^3} \frac{-f\left(\vect{r}, t - \frac{|\vect{r} - \vect{r}'|}{c}\right)}{4 \pi |\vect{r} - \vect{r}'|} \d^3 \vect{r'}
    \end{align*}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Vector potential: initial condition problem}
    
    \begin{align*}
        \left( \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2} \right) \vect{A}(\vect{r}, t) &= - \vect{J}(\vect{r}, t)
    \end{align*}
    
    For initial conditions, we have the ``retarded potential'':
    \begin{align*}
        \vect{A}(\vect{r}, t) &= \int_{\mathbb{R}^3} \frac{\vect{J}\left(\vect{r}, t - \frac{|\vect{r} - \vect{r}'|}{c}\right)}{4 \pi |\vect{r} - \vect{r}'|} \d^3 \vect{r'}
    \end{align*}

    \begin{align*}
        \ft{\vect{A}}(\vect{r}, \omega) &= \int_{\mathbb{R}^3} \ft{\vect{J}}\left(\vect{r}, \omega \right) \frac{e^{- j \frac{\omega}{c} | \vect{r} - \vect{r}' |}}{4 \pi |\vect{r} - \vect{r}'|} \d^3 \vect{r'}
    \end{align*}
    
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Vector potential: final value problem}
    
    \begin{align*}
        \left( \nabla^2 - \frac{1}{c^2} \frac{\partial^2}{\partial t^2} \right) \vect{A}(\vect{r}, t) &= - \vect{J}(\vect{r}, t)
    \end{align*}
    
    For final conditions, we have the ``advanced potential'':
    \begin{align*}
        \vect{A}(\vect{r}, t) &= \int_{\mathbb{R}^3} \frac{\vect{J}\left(\vect{r}, t + \frac{|\vect{r} - \vect{r}'|}{c}\right)}{4 \pi |\vect{r} - \vect{r}'|} \d^3 \vect{r'}
    \end{align*}

    Violation of causality?
    
\end{frame}

\note{}
\fi

\section{Conclusion}
\label{sec:conclusion}

\note{}

\begin{frame}[fragile]
    \frametitle{Takeaways}

    Introduction:
    \begin{itemize}
    \item
        The Green's function is the impulse response
    \item
        Use it to construct solutions. Ignoring boundary conditions:
        \begin{align*}
            u(x) &= \int f(x') G(x,x') \d x'
        \end{align*}
    \end{itemize}
    
\end{frame}

\note{}

\ifextended
\begin{frame}[fragile]
    \frametitle{Takeaways}

    Generalized functions:
    \begin{itemize}
    \item
        The delta function is an operator, not an actual function.
    \end{itemize}
   
\end{frame}

\note{}
\fi

\begin{frame}[fragile]
    \frametitle{Takeaways}
     
    Direct solution:
    \begin{itemize}
    \item
        $ G(x,x') $ obeys source-free equation for $ x \neq x' $.
    \item
        $ G(x,x') $ is continuous at $ x = x' $.
    \item
        $ \dfrac{\d G(x,x')}{\d x} $ is discontinuous at $ x = x' $.
    \end{itemize}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Takeaways}

    Boundary conditions:
    \begin{itemize}
    \item
        Non-zero boundary conditions act like impulse sources.
    \item
        Best way to construct $ u(x) $ from $ G(x,x') $ is to use the adjoint problem.
    \end{itemize}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Takeaways}
    
    Properties of Green's functions:
    \begin{itemize}
    \item
        Self-adjoint problems: $ G(x,x') = G^*(x',x) $ (reciprocity).
    \item
        Invariant problems: $ G(x,x') = G(x - x') $.
    \end{itemize}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Takeaways}
    
    Spectral methods:
    \begin{itemize}
    \item
        Green's function can be found from eigenfunctions/eigenvalues:
        \begin{align*}
            G(x,x') &= \sum_n \frac{\phi_n(x) \phi_n^*(x')}{\lambda_n - \lambda}
        \end{align*}
    \item
        Eigenfunctions/eigenvalues can be found from Green's function.
    \item
        Delta function can be written in terms of eigenfunctions:
        \begin{align*}
            \delta(x - x') &= \sum_n \phi_n(x) \phi_n^*(x')
        \end{align*}
    \end{itemize}
    

\end{frame}

\note{}

\ifextended
\begin{frame}[fragile]
    \frametitle{Takeaways}
    
    The 3D wave equation:
    \begin{itemize}
    \item
        In general, use adjoint problem to write $ u(\vect{r}, t) $ in terms of $ G(\vect{r}, t; \vect{r}', t') $.
    \item
        For free space initial condition problem, solution is the so-called ``retarded potential.''
    \end{itemize}
    
\end{frame}

\note{}
\fi

\begin{frame}[fragile]
    \frametitle{Further topics}

    \begin{itemize}
    \item
        Sturm-Liouville problems
    \item
        Complex contour integration
    \item
        3D problems: separation of variables
    \item
        Dyadic Green's functions
    \ifextended
    \else
    \item
        Generalized functions (delta function)
    \fi
    \end{itemize}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Introductory resources}
    Balanis (2012), \emph{Advanced engineering electromagnetics}. 
    Less rigorous, but good for getting the key ideas.

    Folland (1992), \emph{Fourier analysis and its applications}. 
    Fully rigorous. 
    Chapter on generalized functions is particularly nice.

    Dudley (1994), \emph{Mathematical foundations for electromagnetic theory}.
    Great introduction to 1D Green's functions: deals with subtleties that others ignore.

    Byron and Fuller (1992), \emph{Mathematics of classical and quantum physics}.
    Interesting alternative approach.
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Advanced resources}
    Collin (1990), \emph{Field theory of guided waves}. 
    Huge chapter on Green's functions. Emphasis on dyadics.

    Morse and Feshback, \emph{Methods of theoretical physics}.
    Another big, detailed reference. Great resource for deeper insight and understanding.

    Warnick (1996), ``Electromagnetic Green functions using differential forms.''
    For the differential forms inclined.

\end{frame}    

\note{}

\end{document}
