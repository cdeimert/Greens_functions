% rubber: set program xelatex

% The theme used for this presentation is matze's mtheme, which can be
% found at https://github.com/matze/mtheme

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THINGS TO TALK ABOUT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% Question: Green's function is NOT unique, so can we really treat it as
%           THE response to a source?
%           - Green's function is unique, given specified BC/IC's
%           - So we could say it's the response to the source, under the
%             the imposed BC/IC's
%           - But does this weaken the interpretation of BC/IC's as sources?
%           - Not really: It's the homogeneous BC's that make the Green's function unique
%
%
% Direct solution
%   - Discontinuity condition
%
% Eigenvalue solution
%
% Boundary conditions
%   - Green's function not arbitrary! Depends on BC/IC's.
%   - Idea of replacing BC/IC's with equivalent sources (Morse and Feshback)
%   - Relation to equivalence principle?
%   - Can also just find any Green's and add homogeneous solutions
%   - Can also solve adjoint Green's function problem
%
% Interpreting the Green's function
%   - Space-time invariance
%   - Self-adjointness (IC problems are not self-adjoint!)
%   - Reciprocity (weird combination of invariance/self-adjointness?)
%   - Poles are eigenvalues
%
% Wave equation causality
%   - Causal Green's functions comes from initial conditions in time domain
%   - Equivalent to assuming small loss (a la Harrington)
%   - Can replace initial conditions with equivalent sources
%   - Fourier transform actually fails here!
% 
% Scattering
%   - Interesting approximation technique (Born): the field itself is the source
% Perturbation theory?
% Huygens principle and the propagator?
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newif\ifhandout
\newif\ifextended

\handoutfalse
\handouttrue

\extendedfalse
\extendedtrue

\ifhandout
    \documentclass[12 pt, compress, handout, intlimits]{beamer}

    \setbeamertemplate{note page}[plain]

    \setbeameroption{show notes}% on second screen=bottom}
\else

    \documentclass[12 pt, compress, intlimits]{beamer}

\fi

\usetheme{m}

\usepackage{mymacros}
\usepackage[retainorgcmds]{IEEEtrantools}
\setlength{\IEEEnormaljot}{9pt}

\renewcommand{\d}{\operatorname{d}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\B}{\mathcal{B}}
\newcommand{\inprod}[2]{\left\langle {#1}, {#2} \right\rangle}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{array}
\usepackage{tabularx}

\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{empheq}

\renewcommand{\tabularxcolumn}[1]{>{\normalsize}m{#1}}

\newcommand{\highlight}[1]{\colorbox{mLightBrown!65}{$\displaystyle{#1}$}}
\newcommand{\mygreenbox}[1]{\colorbox{mLightBrown!65}{\hspace{1em}#1\hspace{1em}}}
\newcommand*\widefbox[1]{\fbox{\hspace{1em}#1\hspace{1em}}}

%\usepackage{sourcesanspro}

\usepgfplotslibrary{dateplot}

%\usefonttheme[onlymath]{serif}
%\usepackage{eulervm}
%\usepackage{arevmath}
%\renewcommand{\vect}[1]{\vec{#1}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\ft}[1]{\tilde{#1}}

\useinnertheme{circles}

\setbeamercovered{transparent}

\title{Green's functions}
\subtitle{A short introduction}
\date{\today}
\author{Chris Deimert}
\institute{Department of Electrical and Computer Engineering, University of Calgary}

\begin{document}

\maketitle

\note{
    \begin{itemize}
    \item
        This is intended as a small (dense) overview of Green's functions for electrical engineers.
    \item
        Basic idea of Green's functions is simple, but there is a huge amount of theory for actually calculating and using them.
    \item
        Tip: read a lot of different references. 
        Different authors take totally different approaches and it's interesting to see them all.
        Lots of references are provided at the end.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\note{}

\section{Introduction}
\label{sec:introduction}

\note{
\begin{itemize}
\item
    Fortunately, the basic idea of Green's functions is really simple.
\item
    You've actually used them before!
\item
    What's far more interesting is how to calculate/interpret them.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{What is a Green's function?}
    
    Linear equation to solve:
    \begin{align*}
        \L u(x) &= f(x)
    \end{align*}
    
    \pause

    Green's function is the \textbf{impulse response}:
    \begin{align*}
        \L G(x,x') &= \delta(x - x')
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Most EM problems are described by linear (differential) equations with some source/driving function $ f(x) $.
\item
    The Green's function is the solution when the source $ f(x) $ is an impulse located at $ x' $.
\item
    Can think of it as a generalization of the impulse response from signal processing.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Why is it useful?}
    
    \begin{align*}
        \delta(x - x') &\xrightarrow{\quad \L^{-1} \quad} G(x,x')
    \end{align*}
 
    \pause

    \begin{align*}
        f(x) = \int \delta(x - x') f(x') \d x \xrightarrow{\quad \L^{-1} \quad} \int G(x,x') f(x') \d x
    \end{align*}
  
    %Can find the solution directly for any $ f(x) $:
    %\begin{align*}
    %    u(x) &= \int G(x,x') f(x') \d x
    %\end{align*}
    %\begin{align*}
    %    \L u(x) &= \int \L G(x,x') f(x') \d x = \int \delta(x - x') f(x) \d x = f(x)
    %\end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Once we know the Green's function for a problem, we can find the solution for any source $ f(x) $.
\item
    Impulses $ \delta(x - x') $ produce a response $ G(x,x') $.
\item
    We can split the source $ f(x) $ up into a sum (integral) of impulses $ \delta(x - x') $.
\item
    Then the response to $ f(x) $ is just a weighted sum (integral) of impulse responses.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Why is it useful?}

    \begin{align*}
        \L u(x) &= f(x)
    \end{align*}
    \begin{align*}
        \L G(x,x') &= \delta(x - x')
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        u(x) = \int G(x,x') f(x') \d x
    \end{empheq}

\end{frame}

\note{
\begin{itemize}
\item
    Once we know the Green's function, we have an explicit formula for the solution $ u(x) $ for any source function $ f(x) $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}

    Impulse response of a LTI system:
    \begin{align*}
        y(t) &= \int_{-\infty}^{\infty} x(t') \alert{h(t - t')} \d t'
    \end{align*}

    \pause
    E.g., for an RL-circuit:
    \begin{align*}
        G(t,t') &= h(t - t') = u(t - t') e^{-\alpha (t - t')}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    In electrical engineering, we've seen Green's functions before.
\item
    Impulse response $ h(t - t') $ from linear system theory is an example of a Green's function.
    \begin{align*}
        G(t,t') & = h(t - t')
    \end{align*}
\item
    Usually find $ h(t - t') $ using Fourier transform of the transfer function.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}

    Poisson's equation:
    \begin{align*}
        \nabla^2 V(\vect{r}) &= - \frac{\rho(\vect{r})}{\epsilon_0}
    \end{align*}
    \begin{align*}
        V(\vect{r}) &= \iiint \alert{\frac{1}{4 \pi \epsilon_0 \left| \vect{r} - \vect{r}' \right|^2}} \rho(\vect{r}') \d^3 \vect{r}'
    \end{align*}
\end{frame}

\note{
    \begin{itemize}
    \item
        Green's function for Poisson's equation is
        \begin{align*}
            G(\vect{r}, \vect{r}') &= \frac{1}{4 \pi \epsilon_0 \left| \vect{r} - \vect{r}' \right|^2}
        \end{align*}
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}
    
    Helmholtz equation:
    \begin{align*}
        \left( \nabla^2 + k^2 \right) A_z(\vect{r}) &= -J_z(\vect{r})
    \end{align*}
    \begin{align*}
        A_z(\vect{r}) &= \iiint \alert{\frac{e^{-jk \left| \vect{r} - \vect{r}' \right|}}{4 \pi \left| \vect{r} - \vect{r}' \right|}} J_z\left( \vect{r}' \right) \d^3 \vect{r}'
    \end{align*}
\end{frame}

\note{
    \begin{itemize}
    \item
        Green's function for the Helmholtz equation is
        \begin{align*}
            G(\vect{r}, \vect{r}') &= \frac{e^{-jk \left| \vect{r} - \vect{r}' \right|}}{4 \pi \left| \vect{r} - \vect{r}' \right|}
        \end{align*}
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}
    
    Our goal:
    \begin{itemize}
    \item
        Derive these expressions.
    \item
        Generalize to other problems and boundary conditions.
    \end{itemize}
    
\end{frame}

\note{
}

\section{Generalized functions}
\label{sec:generalized_functions}

\note{
    \begin{itemize}
    \item
        Delta functions play a key role in Green's functions (and electrical engineering in general), but tend to lead to hand-waving.
    \item
        Worth seeing how they can be rigorously defined before moving on.
    \item
        Machinery for this is Schwartz's theory of distributions (generalized functions).
    \item
        See Folland (1992), \emph{Fourier analysis and its applications}, Chapter 9 for more.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Typical delta function definition}
    
    Typical ``definition'' of $ \delta(x - x_0) $:
    \begin{align*}
        \delta(x-x_0) &= 0 \quad \text{for} \quad x \neq x_0
    \end{align*}
    \begin{align*}
        \int_{-\infty}^{\infty} \delta(x-x_0) &= 1
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Often see definitions like this one.
\item
    Often said to imply that $ \delta(x - x_0) = \infty $ at $ x = x_0 $.
\item
    Might be okay intuitively, but very imprecise mathematically.
\item
    There is no true function which satisfies both of these requirements!
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Generalized functions}
    
    $ f(x) $ defines a linear operator $ \phi(x) $ via
    \begin{align*}
        f[\phi] &= \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Let's see if we can generalize the idea of a ``function'' so that it includes delta functions.
\item
    Given a function $ f(x) $, we can use it to define a linear operator (a functional, to be exact) on other functions $ \phi(x) $.
\item
    $ f[\cdot] $ is a linear operator. It takes a function $ \phi(x) $ and returns the number
    \begin{align*}
        f[\phi] &= \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
\item
    If we ensure that $ \phi(x) $ is very well-behaved, then every function $ f(x) $ defines an operator in this way.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Generalized functions}

    If we have $ f[\phi] $, but no $ f(x) $, then $ f $ is a generalized function.
    
    \textbf{Symbolically}, we write
    \begin{align*}
        f[\phi] &\stackrel{s}{=} \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        It's possible to have an operator $ f[\phi] $, but we can't find an $ f(x) $ to implement it via an integral.
    \item
        Then $ f(x) $ is a generalized function. It is not a function in its own right, but it is defined purely by its action on other functions $ f[\phi] $.
    \item
        We still symbolically write
        \begin{align*}
            f[\phi] &\stackrel{s}{=} \int_{-\infty}^{\infty} f(x) \phi(x) \d x
        \end{align*}
        but this just suggestive notation. 
        It is not actually an integral unless $ f(x) $ is a ``proper'' function!
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Defining the delta function}

    $ \delta(x-x_0) $ is a generalized function defined by the sifting property
    \begin{align*}
        \delta_{x_0}[\phi] &= \phi(x_0) \stackrel{s}{=} \int_{-\infty}^{\infty} \delta(x - x_0) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We can define a simple linear operator via the sifting property $ \delta_{x_0}[\phi] = \phi(x_0) $.
\item
    There is no actual function $ \delta(x - x_0) $ which gives
    \begin{align*}
        \int_{-\infty}^{\infty} \delta(x - x_0) \phi(x) \d x &= \phi(x_0)
    \end{align*}
    so $ \delta(x - x_0) $ is a generalized function and the above integral is purely symbolic.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function derivatives}
    
    We can define derivatives too:
    \begin{align*}
        \delta^{(n)}_{x_0}[\phi] &= (-1)^n \phi^{(n)}(x_0) \stackrel{s}{=} \int_{-\infty}^{\infty} \delta^{(n)}(x - x_0) \phi(x) \d x
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Generalized function theory lets us make sense of the derivatives of the delta function too.
\item
    $ \delta_{x_0}^{(n)} $ is just an operator that picks out the value of the $ n $th derivative of $ \phi(x) $ at the point $ x_0 $.
\end{itemize}
}

\ifextended
\begin{frame}[fragile]
    \frametitle{Delta function limits}
    
    \begin{align*}
        \lim_{\epsilon \to 0} f_\epsilon(x) &= \delta(x)
    \end{align*}
    if and only if
    \begin{align*}
        \lim_{\epsilon \to 0} f_\epsilon[\phi] = \lim_{\epsilon \to 0} \int_{-\infty}^{\infty} f_\epsilon(x) \phi(x) \d x &= \phi(0)
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Often useful to show that some set of actual functions $ f_\epsilon(x) $ ``approach'' the delta function in a limit.
\item
    To do this, we need to show that the sifting property is obeyed in the limit.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function limits}
    Limit of Gaussian functions:
    \begin{align*}
        \delta(x) &= \lim_{\epsilon \to 0} \frac{1}{\sqrt{2 \pi} \epsilon} e^{- x^2/ 2 \epsilon^2}
    \end{align*}
    Limit of Lorentzian functions:
    \begin{align*}
        \delta(x) &= \lim_{\epsilon \to 0} \frac{1}{\pi} \frac{\epsilon}{t^2 + \epsilon^2}
    \end{align*}
\end{frame}

\note{
\begin{itemize}
\item
    Two examples of delta function limits.
\item
    Confirms our intuition of the delta function as a limit of sharply-peaked functions.
\item
    In fact, basically any limit of sharply-peaked functions of area 1 will work: see Folland (Theorem 9.2).
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function limits}
    A more interesting example:
    \begin{align*}
        \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{j x t} \d t &= \delta(x)
    \end{align*}
    because
    \begin{align*}
        \lim_{\epsilon \to 0} \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{-\epsilon^2 t^2} e^{j x t} \d t = \delta(x)
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Example of a common, but unintuitive expression for the Delta function.
    \item
        Can show that it's true by expressing it as a delta function limit. (If you want to go through it, use Theorem 9.2 from Folland.)
    \end{itemize}
    
}
\fi

\begin{frame}[fragile]
    \frametitle{What does this mean for Green's functions?}
    
    \begin{align*}
        \L G(x, x') &= \delta(x - x')
    \end{align*}
    actually means
    \begin{align*}
        \left( \L G \right)[\phi]  &= \phi(x') \stackrel{s}{=} \int_{-\infty}^\infty \left( \L G(x, x')\right) \phi(x) \d x
    \end{align*}
\end{frame}

\note{
\begin{itemize}
\item
    Technically, the Green's function is a generalized function such that $ \L G $ is the delta function (it has the sifting property).
\item
    In practise, we'll keep using the less-precise way; just remember that there is a more correct way.
\end{itemize}

}

\begin{frame}[fragile]
    \frametitle{Takeaway}
    
    \begin{center}
        If in doubt, think of $ \delta(x - x_0) $ as an operator, not a function!
    \end{center}

\end{frame}

\note{
\begin{itemize}
\item
    In practise, thinking of $ \delta(x - x_0) $ as a function is usually fine.
    (We'll even do that for the rest of this presentation.)
\item
    But if anything starts to seem fishy, it's good to remember that $ \delta(x - x_0) $ is actually an operator, and not a function.
\end{itemize}
    
}

\section{Direct solution}
\label{sec:direct_solution}

\note{
    \begin{itemize}
    \item
        Back to Green's functions!
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    Original problem:
    \begin{align*}
        \frac{\d^2 u(x)}{\d x^2} - k^2 u(x) &= f(x)
    \end{align*}
    
    Green's function problem:
    \begin{align*}
        \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') &= \delta(x - x')
    \end{align*}
    

\end{frame}

\note{
\begin{itemize}
\item
    Let's look at a simple example now.
\item
    This problem is similar to a simple harmonic oscillator, but the negative sign means we expect lossy behaviour rather than oscillation.
\item
    We won't worry much about boundary conditions yet, we'll just look for solutions that don't blow up at $ x = \pm \infty $.
\item
    If we can find the Green's function, then we can find the solution to the original problem.
\item
    But the Green's function problem looks pretty hard. The point of this example is to demonstrate that we can actually solve it.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    For $ x \neq x' $
    \begin{align*}
        \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') &= 0
    \end{align*}
    
    \pause
    So we have
    \begin{align*}
        G(x,x') &= 
        \begin{cases} 
            A e^{+k (x - x')} & \text{for } x < x'
            \\
            B e^{-k (x - x')} & \text{for } x > x'
        \end{cases}
    \end{align*}
    
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Key thing to notice is that the source is concentrated at $ x = x' $.
    \item
        So for $ x > x' $ and $ x < x' $, we expect the solutions to look like those of the source-free equation.
    \item
        To keep the solutions finite, we expect exponential growth before $ x = x' $ and exponential decay afterward.
    \item
        Now, how do we find the constants $ A $ and $ B $?
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    \begin{align*}
        \frac{\d^2 G(x,x')}{\d x^2} - k^2 G(x,x') &= \delta(x - x')
    \end{align*}

    Continuity of the Green's function:
    \begin{align*}
        \lim_{\epsilon \to 0} \left[ G(x'+\epsilon, x') - G(x' - \epsilon, x') \right] = 0
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    How continuous do we expect our Green's function to be?
\item
    If $ G(x,x') $ is discontinuous (like a step function), then $ \d G / \d x $ will behave like a delta function and $ \d^2 G / \d x^2 $ will behave like a delta function derivative. No good!
\item
    So we expect $ G(x,x') $ to be continuous.
\item
    That gives us one condition we can use to find $ A $ and $ B $. (In fact, it tells us that $ A = B $.)
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}
 
    \begin{align*}
        \alt<2->{\int_{x'-\epsilon}^{x'+\epsilon} \left[ \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') \right] \d x &= \int_{x'-\epsilon}^{x'+\epsilon} \delta(x - x') \d x}{\frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') = \delta(x - x')}
    \end{align*}

    Discontinuity condition:
    \begin{align*}
        \lim_{\epsilon \to 0} \left[ \left. \frac{\d G}{\d x}\right|_{x = x' + \epsilon} - \left. \frac{\d G}{\d x} \right|_{x = x' - \epsilon} \right] &= 1
    \end{align*}
    

\end{frame}

\note{
    \begin{itemize}
    \item
        But what if the derivative $ \d G / \d x $ is discontinuous?
    \item
        Then $ \d^2 G / \d x^2 $ is like a delta function.
        But that's fine, because we have a delta function on the right hand side too.
    \item
        We can find exactly how discontinuous the derivative is by integrating over a small interval around $ x' $.
    \item
        In the limit of $ \epsilon \to 0 $, the second integral vanishes because $ G(x,x') $ is continuous.
    \item
        But, we expect $ \d G / \d x $ to be discontinous. 
    \item
        Using fundamental theorem of calculus, we get at a \emph{discontinuity condition for the derivative.} (Key idea for the direct solution method!)
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    \begin{align*}
        G(x,x') &= 
        \begin{cases} 
            A e^{+k (x - x')} & \text{for } x < x'
            \\
            B e^{-k (x - x')} & \text{for } x > x'
        \end{cases}
    \end{align*}
    
    Continuity of $ G $:
    \begin{align*}
        A &= B
    \end{align*}

    Discontinuity of $ \frac{\d G}{\d x} $:
    \begin{align*}
        k A + k B &= 1
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Applying our two conditions, we can solve for $ A $ and $ B $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}
    At last, our Green's function is
    \begin{align*}
        G(x,x') &= 
        \begin{dcases} 
            \dfrac{e^{+k (x - x')}}{2k} & \text{for } x < x'
            \\
            \dfrac{e^{-k (x - x')}}{2k} & \text{for } x > x'
        \end{dcases}
    \end{align*}
    or, more compactly
    \begin{empheq}[box=\widefbox]{align*}
        G(x, x') = \frac{e^{k |x - x'|}}{2 k}
    \end{empheq}
    

\end{frame}

\note{
}

\begin{frame}[fragile]
    \frametitle{A simple example}
    Solution:
    \begin{align*}
        u(x) &= \int_{-\infty}^{\infty} f(x') \frac{e^{k|x - x'|}}{2k} \d x'
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Now that we have the Green's function, we can construct the solution to our original problem for any forcing function $ f(t) $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{General approach}
    Properties of $ G(x,x') $:
    \begin{itemize}
    \item
        Behaves like source-free solution except at $ x = x' $.
    \item
        Function is continuous at $ x = x' $.
    \item
        Derivative is discontinuous at $ x = x' $.
    \end{itemize}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Listed are the key things to note from that example.
    \item
        This approach works quite well for solving 1D Green's function problems.
    \end{itemize}
}

\section{Boundary conditions}
\label{sec:boundary_conditions}

\note{
    \begin{itemize}
    \item
        We didn't worry about boundary conditions in the last example.
    \item
        As it turns out, Green's functions allow us to deal with boundary conditions in an elegant way.
    \item
        Unfortunately, to derive it, we either have to do a lot of hand-waving or a lot of math. 
        We're going to do a lot of math.
    \item
        See Dudley's \textit{Mathematical foundations for electromagnetic theory} for a more thorough discussion.
    \end{itemize}
}

%\begin{frame}[fragile]
%    \frametitle{Sturm Liouville Problems}
%    
%    Differential equation:
%    \begin{align*}
%        \underbrace{\left( - \frac{1}{w(x)} \frac{\d}{\d x} \left[ p(x) \frac{\d}{\d x} \right] + q(x) - \lambda \right)}_{\displaystyle \L_\lambda} u(x) &= f(x)
%    \end{align*}
%    Boundary conditions:
%    \begin{align*}
%        B_1(u) &= \alpha_{11} u(a) + \alpha_{12} u'(a) + \alpha_{13} u(b) + \alpha_{14} u'(b) =  \alpha
%        \\
%        B_2(u) &= \alpha_{21} u(a) + \alpha_{22} u'(a) + \alpha_{23} u(b) + \alpha_{24} u'(b) =  \beta
%    \end{align*}
%    
%\end{frame}
%
%\note{
%    \begin{itemize}
%    \item
%        This is a general Sturm-Liouville problem.
%    \end{itemize}
%    
%}

\begin{frame}[fragile]
    \frametitle{Adjoint operators}

    \begin{IEEEeqnarray*}{rCrClCrCl}
        \text{Original:} & \qquad & \L [u(x)] &=& f(x); &\qquad& \B [u(x)] &=& 0
        \\
        \text{Adjoint:} & \qquad & \L^* [v(x)] &=& f(x); &\qquad& \B^* [v(x)] &=& 0
    \end{IEEEeqnarray*}
    
    Defining property:
    \begin{align*}
        \inprod{\L u}{v} = \inprod{u}{\L^* v} \qquad \text{where } \inprod{u}{v} = \int_{a}^{b} u(x) v^*(x) \d x
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        To really make sense of boundary conditions, we need the concept of the adjoint problem.
    \item
        Suppose we have an original problem defined by operator $ \L $ and boundary conditions $ \B $.
    \item
        Then, $ \L^* $ is the adjoint operator and $ \B^* $ are the adjoint boundary conditions if $ \inprod{\L u}{v} = \inprod{u}{\L^* v} $ for all $ u, v $.
    \item
        Here $ \inprod{u}{v} $ is the inner product as defined on the slide. ($ v^*(x) $ is the complex conjugate of $ v(x) $.)
    \end{itemize}
}

\note{
    Linear algebra notes:
    \begin{itemize}
    \item
        The boundary conditions are important because they specify the domains of $ \L $ and $ \L^* $. (I.e., $ \L $ operates on the Hilbert space of functions $ u(x) $ which satisfy $ \B[u] = 0 $.)
    \item
        So if $ \B \neq \B^* $, then $ \L $ and $ \L^* $ are operators on different Hilbert spaces.
    \item
        If both $ \L = \L^* $ and $ \B = \B^* $, we say that $ \L $ is self-adjoint.
    \item
        If $ \L = \L^* $ but $ \B \neq \B^* $, we say that $ \L $ is \emph{formally} self-adjoint.
    \end{itemize}
}

\note{
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    \begin{align*}
        \L[u(x)] &= \left[ \frac{\d^2}{\d x^2} + k^2 \right] u(x)
        \\
        \B[u(x)] &= \mat{u(a) \\ u(b)} = \mat{0 \\ 0}
    \end{align*}

    Want $ \L^* $ and $ \B^* $ so that
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v}
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Let's look at an example: the 1D simple harmonic oscillator.
    \item
        We'll use boundary conditions so that $ u(a) = u(b) = 0 $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    
    \vspace{-36pt}
    \begin{align*}
        \inprod{\L u}{v} &= \int_a^b \left[ u''(x) + k^2 u(x) \right] v^*(x) \d x
        %\pause
        %\\
        %&= \int_a^b \left[ - u'(x) v^{*\prime}(x) + k^2 u(x) v^*(x) \right] \d x + \left[ u'(x) v^*(x) \right]_{a}^{b}
        \pause
        \\
        \inprod{\L u}{v} &= \int_a^b u(x) \left[ v^{\prime\prime}(x) + (k^2)^* v(x) \right]^* \d x + 
        \\& \quad + \left[ u'(x) v^*(x)  - u(x) v^{\prime *}(x) \right]_{a}^{b}
    \end{align*}
    By inspection:
    \begin{align*}
        \L^* v(x) &= \left[ \frac{\d^2}{\d x^2} + (k^2)^* \right] v(x)
    \end{align*}
    

\end{frame}

\note{
    \begin{itemize}
    \item
        Integrate by parts twice. 
    \item
        The remaining integral term looks like
        \begin{align*}
            \int_a^b u(x) \left[ \L^* v(x) \right]^* \d x 
        \end{align*}
        so let's define $ \L^* $ this way, and hope it will work out (it will).
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + \left[ u'(x) v^*(x)  - u(x) v^{*\prime}(x) \right]_{a}^{b}
    \end{align*}

    Since $ u(a) = u(b) = 0 $,
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + u'(a) v^*(a) - u'(b) v^*(b)
    \end{align*}

    So, pick
    \begin{align*}
        v(a) = v(b) = 0
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        We almost have the required $ \inprod{\L u}{v} = \inprod{u}{\L^* v} $, but we need the part on the right (called the conjunct) to be zero.
    \item
        From the original problem, we have
        \begin{align*}
            \B[u] &= \mat{u(a) \\ u(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        To make the conjunct zero, we need the adjoint boundary conditions to be
        \begin{align*}
            \B^*[v] &= \mat{v(a) \\ v(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        So in this case, $ \B = \B^* $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
     
    What if $ u(a) = u'(a) = 0 $ (initial conditions)?
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + \left[ u'(x) v^*(x)  - u(x) v^{*\prime}(x) \right]_{a}^{b}
        \\
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + u'(b) v^*(b)  - u(b) v^{\prime *}(b)
    \end{align*}
    We must have \emph{final} conditions for $ v $:
    \begin{align*}
        v(b) = v'(b) = 0
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        What if we use the same operator $ \L $, but we switch from a boundary value problem to an initial condition problem?
        That is,
        \begin{align*}
            \B[u] &= \mat{u(a) \\ u'(a)} = \mat{0 \\ 0}
        \end{align*}
    \item
        Then, to make the conjunct zero, we need the adjoint boundary conditions to be
        \begin{align*}
            \B[v] &= \mat{v(b) \\ v'(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        For an initial condition problem, the adjoint problem is a \emph{final} condition problem!
        $ \B \neq \B^* $. 
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators}

    In general:
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + J(u,v) \Big|_a^b
    \end{align*}
    Definition of adjoint boundary conditions:
    \begin{align*}
        \B[u] = \B^*[v] = 0 \Longleftrightarrow J(u,v) \Big|_a^b = 0
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    In that example, we saw that we always had $ \inprod{\L u}{v} $ equal to $ \inprod{u}{\L^* v} $ plus a leftover term which depended on the boundaries.
\item
    This is true more generally: if we don't specify the boundary conditions of $ u $ and $ v $, then we can still \emph{almost} get the adjoint operator equation. We just have a leftover ``conjunct'' term $ J(u,v) |_a^b $, which depends only on the boundary condtions.
\item
    We define adjoint boundary conditions as those boundary conditions which make the conjunct equal to zero.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint Green's functions}
 
    Original problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L [u(x)] &=& f(x); &\qquad& \B [u(x)] &=& \alpha 
    \end{IEEEeqnarray*}
    Green's problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L [G(x,x')] &=& \delta(x - x'); &\qquad& \B [G(x,x')] &=& 0
    \end{IEEEeqnarray*}
    Adjoint Green's problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L^* [H(x,x')] &=& \delta(x - x'); &\qquad& \B^* [H(x,x')] &=& 0
    \end{IEEEeqnarray*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Now we'll be able to deal with boundary conditions properly.
\item
    We define $ G(x,x') $ to obey the same equation as $ u(x) $, but with $ f(x) \to \delta(x - x') $ and $ \alpha \to 0 $.
    As before, $ G(x,x') $ is the impulse response.
\item
    In addition, we define a new function $ H(x,x') $ which is called the adjoint Green's function.
    It obeys the adjoint version of the $ G(x,x') $ equation.
\item
    Warning: a lot of textbooks don't distinguish between $ H(x,x') $ and $ G(x,x') $.
    Quite often, the ``Green's function'' is really the adjoint Green's function.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \vspace{-12pt}
    Original problem:
    \begin{align*}
        \left( \frac{\d^2}{\d x^2} + k^2 \right) u(x) &= f(x); \quad \mat{u(a) \\ u'(a)} = \mat{\alpha \\ \beta}
    \end{align*}

    Green's problem:
    \begin{align*}
        \left( \frac{\d^2}{\d x^2} + k^2 \right) G(x,x') &= \delta(x - x'); \quad \mat{G(a,x') \\ G'(a,x')} = \mat{0 \\ 0}
    \end{align*}

    Adjoint Green's problem:
    \begin{align*}
        \left( \frac{\d^2}{\d x^2} + k^2 \right) H(x,x') &= \delta(x - x'); \quad \mat{H(b,x') \\ H'(b,x')} = \mat{0 \\ 0}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We'll show how $ H(x,x') $ is useful with an example.
\item
    Here we have a driven simple harmonic oscillator with initial conditions.
    (We'll take $ k $ real for simplicity.)
\item
    From before, we know that the adjoint problem will be the same differential equation, but with final conditions instead of initial conditions.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                A \cos(k(x - x')) + B \sin(k(x - x')) & \text{for } x < x'
                \\
                C \cos(k(x - x')) + D \sin(k(x - x')) & \text{for } x > x'
            \end{cases}
    \end{align*}
    \begin{align*}
        \text{Final conditions } & \Longrightarrow C = D = 0
        \\
        \text{Continuity of function } & \Longrightarrow A = 0
        \\
        \text{Discontinuity of derivative } & \Longrightarrow B = \frac{-1}{k}
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        We can solve for $ H(x,x') $ using a similar approach to before.
    \item
        Except at $ x = x' $, we write $ H(x,x') $ as a solution to the source-free equation.
    \item
        Then we find the coefficients using the boundary conditions and continuity/discontinuity requirements.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                - k^{-1} \sin(k (x - x')) & \text{for } x < x'
                \\
                0 & \text{for } x > x'
            \end{cases} 
    \end{align*}
 
\end{frame}

\note{
    \begin{itemize}
    \item
        So we have our adjoint Green's function.
    \item
        Now we just need to figure out how to construct $ u(x) $ from it.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        \inprod{\L u(x)}{H(x,x')} &= \inprod{u(x)}{\L^* H(x,x')} + J(u, H) \Big|_a^b
        \\
        \inprod{f(x)}{H(x,x')} &= \inprod{u(x)}{\delta(x - x')} + J(u, H) \Big|_a^b
        \\
        \int_a^b f(x) H^*(x,x') \d x &= \int_a^b u(x) \delta(x - x') \d x + J(u, H) \Big|_a^b
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x - J(u, H) \Big|_a^b
    \end{empheq}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        To construct the solution $ u(x) $, we take an inner product of $ \L u(x) $ with $ H(x,x') $, and apply our knowledge of adjoints and conjuncts.
    \item
        We arrive at a fairly general formula which looks close to what we expect a Green's function formula to look like, but with an extra conjunct term.
    \item
        We'll gain insight into the $ J(u,H) $ term by expanding it for this example.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
    Expand $ J(u,H) $:
    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x - \left[ \frac{\d u(x)}{\d x} H^*(x,x') - u(x) \frac{\d H^*(x,x')}{\d x} \right]_a^b
    \end{align*}

    Recall:
    \begin{align*}
        \B[u] = \mat{u(a) \\ u'(a)} = \mat{\alpha \\ \beta}; \qquad \B^*[H] = \mat{H(b,x') \\ H'(b,x')} = \mat{0 \\ 0}
    \end{align*}
    
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Now let's expand $ J(u,v) $ for our particular example. (We can basically copy it from a previous part of the derivation.)
    \item
        We can simplify the conjunct by remembering our boundary conditions.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x + \beta H^*(a,x') - \alpha \frac{\d H^*(a,x')}{\d x}
    \end{align*}
   
\end{frame}

\note{
    \begin{itemize}
    \item
        We're very close to our final goal. 
    \item
        The last thing will be to get rid of $ H(x,x') $ and replace it with $ G(x,x') $.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Adjoint Green's functions}

    How are $ G(x,x') $ and $ H(x,x') $ related?
    \begin{align*}
        \inprod{\L G(x,x')}{H(x,x'')} &= \inprod{G(x,x')}{\L^* H(x,x'')}
        \pause
        \\
        \inprod{\delta(x - x')}{H(x,x'')} &= \inprod{G(x,x')}{\delta(x - x'')}
        \pause
        \\
        \int_a^b \delta(x - x') H^*(x,x'') \d x  &= \int_a^b G(x,x') \delta(x - x'') \d x
        \\
        H^*(x',x'') &= G(x'',x')
    \end{align*}
        \pause
    \begin{empheq}[box=\widefbox]{align*}
        G(x,x') &= H^*(x',x)
    \end{empheq}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Using the definition of the adjoint problem, we find that there is a simple relationship between $ G(x,x') $ and $ H(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
     
    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                - k^{-1} \sin(k (x - x')) & \text{for } x < x'
                \\
                0 & \text{for } x > x'
            \end{cases} 
    \end{align*}

    \begin{align*}
        G(x,x') &= 
            \begin{cases} 
                0 & \text{for } x < x'
                \\
                k^{-1} \sin(k (x - x')) & \text{for } x > x'
            \end{cases} 
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Since we already know the adjoint Green's function $ H(x,x') $, we can use find the Green's function via $ G(x,x') = H^*(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
 
    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x + \beta H^*(a,x') - \alpha \left. \frac{\d H^*(x,x')}{\d x} \right|_{x=a}
        \pause
        \\
        u(x') &= \int_a^b f(x) G(x',x) \d x + \beta G(x',a) - \alpha \left. \frac{\d G(x',x)}{\d x} \right|_{x=a}
        \pause
        \\
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        With our new knowledge that $ G(x,x') = H^*(x',x) $, we can rewrite the solution $ u(x) $ in terms of $ G(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Final solution}

    \begin{empheq}[box=\widefbox]{align*}
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
    \end{empheq}
    where
    \begin{align*}
        G(x,x') &= 
            \begin{cases} 
                0 & \text{for } x < x'
                \\
                k^{-1} \sin(k (x - x')) & \text{for } x > x'
            \end{cases} 
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Finally, we arrive at our solution.
\item
    First, note that the final answer does \emph{not} depend on $ H(x,x') $. We didn't actually need to ever calculate $ H(x,x') $ from the adjoint Green's equation, we could have just found $ G(x,x') $ from the (non-adjoint) Green's equation.
\item
    However, it would have been very difficult to derive this expression without using $ H(x,x') $ (I couldn't see an easy way).
    Because of this, a lot of authors stop at the expression for $ u(x') $ in terms of $ H^*(x,x') $, and they just call $ H^*(x,x') $ the ``Green's function.''
\item
    By doing the extra work, though, we gain a very nice interpretation for the boundary conditions term.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Interpretation of boundary conditions}
    
    \begin{align*}
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
        \\
        u(x) &= \int_a^b \Big[ f(x') + \beta \delta(x' - a) - \alpha \delta'(x' - a) \Big] G(x,x') \d x'
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We note that our expression for $ u(x) $ looks like it did before (integral over $ f(x') G(x,x') $), but now there are extra terms which depend on the boundary conditions.
\item
    We come to a key idea: boundary conditions have the same effect on $ u(x) $ as adding little impulse sources at the boundary.
    The Green's function can deal with both sources $ f(x) $ and non-zero boundary conditions.
\item
    Be careful, though: $ G(x,x') $ still depends on the \emph{type} of boundary condition.
    E.g., we use the same $ G(x,x') $ for all initial value problems ($ u(a), u'(a) $ specified), but we'll need a different $ G(x,x') $ for boundary value problems ($ u(a), u(b) $ specified).
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Summary}
    \vspace{-36pt}
    \begin{IEEEeqnarray*}{rClCrCl}
        \L[u(x)] &=& f(x); &\qquad& \B[u(x)] &=& \alpha
        \\
        \L[G(x,x')] &=& \delta(x - x'); &\qquad& \B[G(x,x')] &=& 0
        \\
        \L^*[H(x,x')] &=& \delta(x - x'); &\qquad& \B^*[H(x,x')] &=& 0
    \end{IEEEeqnarray*}

    \begin{enumerate}
    \item
        Find $ G(x,x') $.
    \item
        Find $ u(x') $ in terms of $ H(x,x') $:
        \begin{align*}
            u(x') &= \int_a^b f(x) H^*(x,x') \d x - J(u(x), H(x,x'))
        \end{align*}
    \item
        Express $ u(x) $ in terms of $ G(x,x')= H^*(x',x) $.
    \end{enumerate}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        That was a long process, so let's summarize what we did.
    \item
        First, we wrote out the original equation, the Green's function equation, and the adjoint Green's function equation. (Take note of the boundary conditions in particular.)
    \item
        Next, we solve the Green's function equation for $ G(x,x') $.
    \item
        Next, we found that it's much easier to express $ u(x) $ in terms of $ H(x,x') $, because we can use inner products.
        The only tricky part is finding the conjunct. 
        (Usually, just requires integration by parts.
        See Dudley for a general formula for Sturm-Liouville problems.)
    \item
        Finally, we replace $ H(x,x') $ with $ G^*(x',x) $ (which we solved for previously), and we have our final expression for $ u(x) $.
    \end{itemize}
}

\section{Properties of Green's functions}
\label{sec:properties_of_green_s_functions}

\note{
    \begin{itemize}
    \item
        The Green's function gives us a lot of information about the system we're dealing with.
    \item
        Here we'll look at a few of the properties Green's functions can have and what those tell us about our system.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Reciprocity}

    If $ \L $ is self-adjoint
    \begin{align*}
        \L = \L^* \qquad \text{and} \qquad \B = \B^*
    \end{align*}
    then
    \begin{align*}
        G(x,x') &= G^*(x',x)
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        For self-adjoint problems, $ G = H $. 
    \item
        Using our relationship between $ G $ and $ H $, we quickly see that the Green's function is complex symmetric (Hermitian) for self-adjoint problems.
    \item
        Roughly, putting a source at $ x $ and measuring at $ x' $ is the same as putting a source at $ x' $ and measuring at $ x $.
        This is often called reciprocity (though it's not quite the same as reciprocity from time-harmonic E\&M, because we're using a true inner product rather than the so-called ``reaction inner product'').
    \item
        Note that we require \emph{both} $ \L = \L^* $ and $ \B = \B^* $. So this would likely not hold for initial condition problems.
    \end{itemize}
}


\begin{frame}[fragile]
    \frametitle{Invariance}


    $ \L $ is invariant if 
    \begin{align*}
        \L[u(x - \xi)] &= \L[u(x)] \Big|_{x = x - \xi}
    \end{align*}
    
    For example,
    \begin{IEEEeqnarray*}{rClCl}
        \L &=& a \frac{\d^2}{\d x^2} + b \frac{\d}{\d x} + c
    \end{IEEEeqnarray*}
    is invariant only if $ a $, $ b $, and $ c $ are constants.
    
    
\end{frame}

\note{
\begin{itemize}
\item
    An operator is invariant if shifting the input just results in a shifted output.
\item
    Probably familiar from signal processing: linear \emph{time-invariant} systems.
    Delaying the input signal leads to the same output signal, just delayed by the same amount.
\item
    Invariance is really important in modern physics.
\item
    Maxwell's equations in free space are invariant with respect to $ x, y, z, \phi, \theta $.
    That is, at a fundamental level, the laws of electromagnetism do not change if we move to a different location or look in a different direction.
\end{itemize}

}

\begin{frame}[fragile]
    \frametitle{Invariance}

    If $ \L $ is invariant in $ x $, then
    \begin{align*}
        \L[G(x,x')] &= \delta(x - x')
        \\
        \L[G(x - \xi,x' - \xi)] &= \delta(x - x')
    \end{align*}
    \begin{align*}
        \Longrightarrow G(x,x') = G(x - \xi, x' - \xi)
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        G(x, x') &= G(x - x')
    \end{empheq}
    
\end{frame}

\note{
\begin{itemize}
\item
    For invariant problems, we can see that shifting both $ x $ and $ x' $ by the same amount $ \xi $ does not affect the Green's function.
\item
    Taking $ \xi  = x' $, we see that $ G(x,x') $ is actually only a function of the difference $ (x - x') $.
\item
    That is, the response only depends on the \emph{relative} locations of the source and measurement.
    This fits nicely with our intuitive understanding of invariance.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Invariance}
    
    Convolution:
    \begin{align*}
        u(x) &= \int_a^b G(x - x') f(x') \d x' = G(x) * f(x)
    \end{align*}
    Frequency domain:
    \begin{align*}
        \ft{u}(k) &= \ft{G}(k) \ft{f}(k)
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    For invariant systems (with boundary conditions zero), the solution $ u(x) $ is just given as a convolution of the source function $ f(x) $ with the impulse response $ G(x) $.
\item
    Taking Fourier transforms, the convolution turns into multiplication. 
\item
    Looks familiar from signal processing!
\end{itemize}
}


\section{Spectral methods}
\label{sec:spectral_methods}

\note{
\begin{itemize}
\item
    In this section, we'll look at the strong relationship between Green's functions and spectral theory.
\item
    Essentially, eigenfunction expansion allows us to calculate the Green's function when direct methods don't work.
\item
    A basic background in spectral theory can be found in most books covering Green's functions. 
\item
    Unfortunately, these are rarely rigorous when dealing with continuous sets of eigenvalues.
    For fully rigorous spectral theory (not for the faint of heart!), see Naylor and Sell's \emph{Linear operator theory in engineering and science} or Kreyszig's \emph{Introductory functional analysis with applications.}
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}

    Problem:
    \begin{align*}
        \left( \L - \lambda \right)u(x) &= f(x); \qquad \B[u(x)] = 0
    \end{align*}
    where $ \L $ is self-adjoint:
    \begin{align*}
        \L = \L^* \quad \text{and} \quad \B = \B^*
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        First we'll review a bit of eigenfunction theory, but we'll quickly see how it relates to Green's functions.
    \item
        Set up a similar problem as before, but we've added a complex parameter $ \lambda $ for later convenience.
    \item
        Also, for this section we'll insist that $ \L $ be fully self-adjoint so that we can take full advantage of spectral theory.
    \item
        A brief discussion of the non-self-adjoint case can be found in Morse and Feshbach under ``Non-Hermitian operators: biorthogonal functions''.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}
    
    Eigenfunctions:
    \begin{align*}
        \L \phi_n(x) &= \lambda_n \phi_n(x)
    \end{align*}
    
    With $ \L $ self-adjoint, $ \lambda_n \in \mathbb{R} $ and
    \begin{align*}
        u(x) &= \sum_n \inprod{u}{\phi_n} \phi_n(x)
        \\
        f(x) &= \sum_n \inprod{f}{\phi_n} \phi_n(x)
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Because $ \L $ is self-adjoint, we know that its eigenvalues $ \lambda_n $ are real.
\item
    We also know that it has a complete orthonormal set of eigenfunctions $ \phi_n $.
\item
    That is, we can expand any function (in this case $ u(x) $ and $ f(x) $) in terms of $ \phi_n(x) $. (Generalized Fourier series.)
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}

    \begin{align*}
        (\L - \lambda)u(x) &= f(x)
        \\
        \pause
        (\L - \lambda) \left[ \sum_n \inprod{u}{\phi_n} \phi_n(x) \right] &= \sum_n \inprod{f}{\phi_n} \phi_n(x)
        \\
        \pause
        \sum_n \inprod{u}{\phi_n} (\lambda_n - \lambda) \phi_n(x) &= \sum_n \inprod{f}{\phi_n} \phi_n(x)
        \\
        \pause
        (\lambda_n - \lambda) \inprod{u}{\phi_n} &= \inprod{f}{\phi_n}
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Going back to our original equation, let's expand $ u(x) $ and $ f(x) $ in terms of eigenfunctions of $ \L $.
    \item
        Using the fact that $ \L $ is linear and $ \L \phi_n = \lambda_n \phi_n $, we can get rid of $ \L $ (third line).
    \item
        Finally, since the $ \phi_n(x) $ are linearly independent, each term in the sums on the RHS and LHS must be equal.
        So we get an expression for the generalized Fourier coefficients $ \inprod{u}{\phi_n} $.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}

    \begin{align*}
        \inprod{u}{\phi_n} &= \frac{\inprod{f}{\phi_n}}{\lambda_n - \lambda}
    \end{align*}

    So
    \begin{align*}
        u(x) &= \sum_n \inprod{u}{\phi_n} \phi_n(x)
        \\
        \Aboxed{u(x) &= \sum_n \frac{\inprod{f}{\phi_n}}{\lambda_n - \lambda} \phi_n(x)}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Plugging in our new expression for the Fourier coefficients, we obtain a formula for $ u(x) $ in terms of the eigenfunctions and eigenvalues of $ \L $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Eigenfunction expansion}

    \vspace{-16pt}
    \begin{align*}
        u(x) &= \sum_n \frac{\inprod{f}{\phi_n}}{\lambda_n - \lambda} \phi_n(x)
        \\
        u(x) &= \sum_n \left( \int_a^b \frac{f(x') \phi^*_n(x')}{\lambda_n - \lambda} \d x' \right) \phi_n(x)
        \\
        u(x) &= \int_a^b \left( \sum_n \frac{f(x) \phi^*_n(x')}{\lambda_n - \lambda} \phi_n(x) \right) \d x'
        \\
        u(x) &= \int_a^b \left( \alert{\sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda} } \right) f(x') \d x'
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Usually, the inner product is defined by an integral.
\item
    If we write this out and do some manipulation, we get something that looks a lot like the Green's function expression.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Spectral form of the Green's function}
    
    \begin{align*}
        u(x) &= \int_a^b \left( \sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda} \right) f(x') \d x'
    \end{align*}

    \begin{empheq}[box=\widefbox]{align*}
        G(x,x') &= \sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda}
    \end{empheq}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        It turns out that we actually can read off this weird sum as the Green's function.
    \item
        So, if we know the eigenvalues and eigenfunctions of $ \L $, we can immediately construct the Green's function as an infinite series.
    \item
        Note also that $ G(x,x') = G^*(x,x') $, as we expect because this is a self-adjoint problem.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Spectral form of the Green's function}

    Green's function of $ (\L - \lambda) $:
    \begin{align*}
        G(x,x',\lambda) &= \sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda}
    \end{align*}

    $ \lambda_n $ are poles of $ G(x,x',\lambda) $.

    $ \phi_n(x) $ can be found by residue integration.
    
\end{frame}

\note{
\begin{itemize}
\item
    It also goes the other way. If we know the Green's function of $ (\L - \lambda) $ for any complex $ \lambda $, then the eigenvalues of $ \L $ are just the poles of the Green's function with respect to lambda.
\item
    Eigenfunctions are a little trickier to read off, but it's possible to find them from the Green's function using residue integration.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Spectral form of the delta function}

    \begin{align*}
        \delta(x - x') &= (\L - \lambda) G(x,x') 
        \pause
        \\
        \delta(x - x') &= (\L - \lambda) \sum_n \frac{\phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda}
        \pause
        \\
        \delta(x - x') &= \sum_n \frac{(\lambda_n - \lambda) \phi_n(x) \phi^*_n(x')}{\lambda_n - \lambda}
        \pause
        \\
        \Aboxed{\delta(x - x') &= \sum_n \phi_n(x) \phi^*_n(x')}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Using our Green's function equation, we can also derive an expression for the delta function as a sum of eigenfunctions.
\item
    This expression is useful when solving three-dimensional problems with separation of variables.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example}

    \begin{align*}
        \underbrace{\left( \frac{\d^2}{\d x^2} - \lambda \right)}_{\displaystyle \L - \lambda} u(x) = f(x); \quad u(0) = u(a) = 0
    \end{align*}

    Eigenfunctions of $ \L $:
    \begin{align*}
        \phi_n(x) &= \sqrt{\frac{2}{a}} \sin\left( \frac{\pi n x}{a} \right); \quad \lambda_n = \frac{\pi n}{a}
    \end{align*}
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Example}
    
    \begin{align*}
        G(x,x') &= \sum_n \frac{\phi_n(x) \phi_n^*(x')}{\lambda_n - \lambda}
        \\
        G(x,x') &= \sum_{n=0}^{\infty} \frac{2 \sin\Big( \dfrac{\pi n x}{a} \Big)\sin\Big( \dfrac{\pi n x'}{a} \Big)}{\pi n - \lambda a}
    \end{align*}
    

\end{frame}

\note{
\begin{itemize}
\item
    Not as good as the direct method (have to add up an infinite sum to calculate numerical values), but this may be necessary for 3D problems.
\end{itemize}
}

\section{3D problems}
\label{sec:3d_problems}

\note{}

\section{Advanced topics}
\label{sec:advanced_topics}

\note{}


\begin{frame}[fragile]
    \frametitle{Introductory resources}
    Balanis (2012), \emph{Advanced engineering electromagnetics}. 
    Less rigorous, but good for getting the key ideas.

    Folland (1992), \emph{Fourier analysis and its applications}. 
    Rigorous. Chapter on generalized functions is particularly nice.

    Dudley (1994), \emph{Mathematical foundations for electromagnetic theory}.
    Great introduction to 1D Green's functions: deals with subtleties that others ignore.

    Byron and Fuller (1992), \emph{Mathematics of classical and quantum physics}.
    Interesting alternative approach.
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Advanced resources}
    Collin (1990), \emph{Field theory of guided waves}. 
    Huge chapter on Green's functions. Emphasis on dyadics.

    Morse and Feshback, \emph{Methods of theoretical physics}.
    Another big, detailed reference. Great resource for deeper insight and understanding.

    Warnick (1996), ``Electromagnetic Green functions using differential forms.''
    For the differential forms inclined.

\end{frame}    

\note{}

\end{document}
