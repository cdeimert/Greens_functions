% rubber: set program xelatex

% The theme used for this presentation is matze's mtheme, which can be
% found at https://github.com/matze/mtheme

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THINGS TO TALK ABOUT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
% Question: Green's function is NOT unique, so can we really treat it as
%           THE response to a source?
%           - Green's function is unique, given specified BC/IC's
%           - So we could say it's the response to the source, under the
%             the imposed BC/IC's
%           - But does this weaken the interpretation of BC/IC's as sources?
%           - Not really: It's the homogeneous BC's that make the Green's function unique
%
%
% Direct solution
%   - Discontinuity condition
%
% Eigenvalue solution
%
% Boundary conditions
%   - Green's function not arbitrary! Depends on BC/IC's.
%   - Idea of replacing BC/IC's with equivalent sources (Morse and Feshback)
%   - Relation to equivalence principle?
%   - Can also just find any Green's and add homogeneous solutions
%   - Can also solve adjoint Green's function problem
%
% Interpreting the Green's function
%   - Space-time invariance
%   - Self-adjointness (IC problems are not self-adjoint!)
%   - Reciprocity (weird combination of invariance/self-adjointness?)
%   - Poles are eigenvalues
%
% Wave equation causality
%   - Causal Green's functions comes from initial conditions in time domain
%   - Equivalent to assuming small loss (a la Harrington)
%   - Can replace initial conditions with equivalent sources
%   - Fourier transform actually fails here!
% 
% Scattering
%   - Interesting approximation technique (Born): the field itself is the source
% Perturbation theory?
% Huygens principle and the propagator?
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newif\ifhandout
\newif\ifextended

\handoutfalse
%\handouttrue

\extendedfalse
\extendedtrue

\ifhandout
    \documentclass[12 pt, compress, handout, intlimits]{beamer}

    \setbeamertemplate{note page}[plain]

    \setbeameroption{show notes}% on second screen=bottom}
\else

    \documentclass[12 pt, compress, intlimits]{beamer}

\fi

\usetheme{m}

\usepackage{mymacros}
\usepackage[retainorgcmds]{IEEEtrantools}
\setlength{\IEEEnormaljot}{9pt}

\renewcommand{\d}{\operatorname{d}}
\renewcommand{\L}{\mathcal{L}}
\renewcommand{\B}{\mathcal{B}}
\newcommand{\inprod}[2]{\left\langle {#1}, {#2} \right\rangle}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage{array}

\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{empheq}

\newcommand{\highlight}[1]{\colorbox{mLightBrown!65}{$\displaystyle{#1}$}}
\newcommand{\mygreenbox}[1]{\colorbox{mLightBrown!65}{\hspace{1em}#1\hspace{1em}}}
\newcommand*\widefbox[1]{\fbox{\hspace{1em}#1\hspace{1em}}}

%\usepackage{sourcesanspro}

\usepgfplotslibrary{dateplot}

%\usefonttheme[onlymath]{serif}
%\usepackage{eulervm}
%\usepackage{arevmath}
%\renewcommand{\vect}[1]{\vec{#1}}
\renewcommand{\L}{\mathcal{L}}

\useinnertheme{circles}

\setbeamercovered{transparent}

\title{Green's functions}
\subtitle{A short introduction}
\date{\today}
\author{Chris Deimert}
\institute{Department of Electrical and Computer Engineering, University of Calgary}


\begin{document}

\maketitle

\note{
    \begin{itemize}
    \item
        This is intended as a quick overview of Green's functions for electrical engineers.
    \item
        Green's functions are a huge subject: it's easy to get overwhelmed by calculation techniques. 
    \item
        Focus here will be on intuition/understanding and awareness of some key techniques.
    \item
        Lots of further reading provided at the end.
    \item
        Tip: read a lot of different references. 
        Different authors take totally different approaches and it's interesting to see them all.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\note{}

\section{Introduction}
\label{sec:introduction}

\note{
\begin{itemize}
\item
    Fortunately, the basic idea of Green's functions is really simple.
\item
    You've actually used them before!
\item
    What's far more interesting is how to calculate/interpret them.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{What is a Green's function?}
    
    Linear equation to solve:
    \begin{align*}
        \L u(x) &= f(x)
    \end{align*}
    
    \pause

    Green's function is the \textbf{impulse response}:
    \begin{align*}
        \L G(x,x') &= \delta(x - x')
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Most EM problems are described by linear (differential) equations with some source/driving function $ f(x) $.
\item
    The Green's function is the solution when the source $ f(x) $ is an impulse located at $ x' $.
\item
    Can think of it as a generalization of the impulse response from signal processing.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Why is it useful?}
    
    \begin{align*}
        \delta(x - x') &\xrightarrow{\quad \L^{-1} \quad} G(x,x')
    \end{align*}
 
    \pause

    \begin{align*}
        f(x) = \int \delta(x - x') f(x') \d x \xrightarrow{\quad \L^{-1} \quad} \int G(x,x') f(x') \d x
    \end{align*}
  
    %Can find the solution directly for any $ f(x) $:
    %\begin{align*}
    %    u(x) &= \int G(x,x') f(x') \d x
    %\end{align*}
    %\begin{align*}
    %    \L u(x) &= \int \L G(x,x') f(x') \d x = \int \delta(x - x') f(x) \d x = f(x)
    %\end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Once we know the Green's function for a problem, we can find the solution for any source $ f(x) $.
\item
    Impulses $ \delta(x - x') $ produce a response $ G(x,x') $.
\item
    We can split the source $ f(x) $ up into a sum (integral) of impulses $ \delta(x - x') $.
\item
    Then the response to $ f(x) $ is just a weighted sum (integral) of impulse responses.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Why is it useful?}

    \begin{align*}
        \L u(x) &= f(x)
    \end{align*}
    \begin{align*}
        \L G(x,x') &= \delta(x - x')
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        u(x) = \int G(x,x') f(x') \d x
    \end{empheq}

\end{frame}

\note{
\begin{itemize}
\item
    Once we know the Green's function, we have an explicit formula for the solution $ u(x) $ for any source function $ f(x) $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}

    Impulse response of a LTI system:
    \begin{align*}
        y(t) &= \int_{-\infty}^{\infty} x(t') \alert{h(t - t')} \d t'
    \end{align*}

    \pause
    E.g., for an RL-circuit:
    \begin{align*}
        G(t,t') &= h(t - t') = u(t - t') e^{-\alpha (t - t')}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    In electrical engineering, we've seen Green's functions before.
\item
    Impulse response $ h(t - t') $ from linear system theory is an example of a Green's function.
    \begin{align*}
        G(t,t') & = h(t - t')
    \end{align*}
\item
    Usually find $ h(t - t') $ using Fourier transform of the transfer function.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}

    Poisson's equation:
    \begin{align*}
        \nabla^2 V(\vect{r}) &= - \frac{\rho(\vect{r})}{\epsilon_0}
    \end{align*}
    \begin{align*}
        V(\vect{r}) &= \iiint \alert{\frac{1}{4 \pi \epsilon_0 \left| \vect{r} - \vect{r}' \right|^2}} \rho(\vect{r}') \d^3 \vect{r}'
    \end{align*}
\end{frame}

\note{
    \begin{itemize}
    \item
        Green's function for Poisson's equation is
        \begin{align*}
            G(\vect{r}, \vect{r}') &= \frac{1}{4 \pi \epsilon_0 \left| \vect{r} - \vect{r}' \right|^2}
        \end{align*}
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}
    
    Helmholtz equation:
    \begin{align*}
        \left( \nabla^2 + k^2 \right) A_z(\vect{r}) &= -J_z(\vect{r})
    \end{align*}
    \begin{align*}
        A_z(\vect{r}) &= \iiint \alert{\frac{e^{-jk \left| \vect{r} - \vect{r}' \right|}}{4 \pi \left| \vect{r} - \vect{r}' \right|}} J_z\left( \vect{r}' \right) \d^3 \vect{r}'
    \end{align*}
\end{frame}

\note{
    \begin{itemize}
    \item
        Green's function for the Helmholtz equation is
        \begin{align*}
            G(\vect{r}, \vect{r}') &= \frac{e^{-jk \left| \vect{r} - \vect{r}' \right|}}{4 \pi \left| \vect{r} - \vect{r}' \right|}
        \end{align*}
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Familiar Green's functions}
    
    Our goal:
    \begin{itemize}
    \item
        Derive these expressions.
    \item
        Generalize to other problems and boundary conditions.
    \end{itemize}
    
\end{frame}

\note{
}

\section{Generalized functions}
\label{sec:generalized_functions}

\note{
    \begin{itemize}
    \item
        Delta functions play a key role in Green's functions (and electrical engineering in general), but tend to lead to hand-waving.
    \item
        Worth seeing how they can be rigorously defined before moving on.
    \item
        Machinery for this is Schwartz's theory of distributions (generalized functions).
    \item
        See Folland (1992), \emph{Fourier analysis and its applications}, Chapter 9 for more.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Typical delta function definition}
    
    Typical ``definition'' of $ \delta(x - x_0) $:
    \begin{align*}
        \delta(x-x_0) &= 0 \quad \text{for} \quad x \neq x_0
    \end{align*}
    \begin{align*}
        \int_{-\infty}^{\infty} \delta(x-x_0) &= 1
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Often see definitions like this one.
\item
    Often said to imply that $ \delta(x - x_0) = \infty $ at $ x = x_0 $.
\item
    Might be okay intuitively, but very imprecise mathematically.
\item
    There is no true function which satisfies both of these requirements!
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Generalized functions}
    
    $ f(x) $ defines a linear operator $ \phi(x) $ via
    \begin{align*}
        f[\phi] &= \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Let's see if we can generalize the idea of a ``function'' so that it includes delta functions.
\item
    Given a function $ f(x) $, we can use it to define a linear operator (a functional, to be exact) on other functions $ \phi(x) $.
\item
    $ f[\cdot] $ is a linear operator. It takes a function $ \phi(x) $ and returns the number
    \begin{align*}
        f[\phi] &= \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
\item
    If we ensure that $ \phi(x) $ is very well-behaved, then every function $ f(x) $ defines an operator in this way.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Generalized functions}

    If we have $ f[\phi] $, but no $ f(x) $, then $ f $ is a generalized function.
    
    \textbf{Symbolically}, we write
    \begin{align*}
        f[\phi] &\stackrel{s}{=} \int_{-\infty}^{\infty} f(x) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        It's possible to have an operator $ f[\phi] $, but we can't find an $ f(x) $ to implement it via an integral.
    \item
        Then $ f(x) $ is a generalized function. It is not a function in its own right, but it is defined purely by its action on other functions $ f[\phi] $.
    \item
        We still symbolically write
        \begin{align*}
            f[\phi] &\stackrel{s}{=} \int_{-\infty}^{\infty} f(x) \phi(x) \d x
        \end{align*}
        but this just suggestive notation. 
        It is not actually an integral unless $ f(x) $ is a ``proper'' function!
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Defining the delta function}

    $ \delta(x-x_0) $ is a generalized function defined by the sifting property
    \begin{align*}
        \delta_{x_0}[\phi] &= \phi(x_0) \stackrel{s}{=} \int_{-\infty}^{\infty} \delta(x - x_0) \phi(x) \d x
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We can define a simple linear operator via the sifting property $ \delta_{x_0}[\phi] = \phi(x_0) $.
\item
    There is no actual function $ \delta(x - x_0) $ which gives
    \begin{align*}
        \int_{-\infty}^{\infty} \delta(x - x_0) \phi(x) \d x &= \phi(x_0)
    \end{align*}
    so $ \delta(x - x_0) $ is a generalized function and the above integral is purely symbolic.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function derivatives}
    
    We can define derivatives too:
    \begin{align*}
        \delta^{(n)}_{x_0}[\phi] &= (-1)^n \phi^{(n)}(x_0) \stackrel{s}{=} \int_{-\infty}^{\infty} \delta^{(n)}(x - x_0) \phi(x) \d x
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    Generalized function theory lets us make sense of the derivatives of the delta function too.
\item
    $ \delta_{x_0}^{(n)} $ is just an operator that picks out the value of the $ n $th derivative of $ \phi(x) $ at the point $ x_0 $.
\end{itemize}
}

\ifextended
\begin{frame}[fragile]
    \frametitle{Delta function limits}
    
    \begin{align*}
        \lim_{\epsilon \to 0} f_\epsilon(x) &= \delta(x)
    \end{align*}
    if and only if
    \begin{align*}
        \lim_{\epsilon \to 0} f_\epsilon[\phi] = \lim_{\epsilon \to 0} \int_{-\infty}^{\infty} f_\epsilon(x) \phi(x) \d x &= \phi(0)
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Often useful to show that some set of actual functions $ f_\epsilon(x) $ ``approach'' the delta function in a limit.
\item
    To do this, we need to show that the sifting property is obeyed in the limit.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function limits}
    Limit of Gaussian functions:
    \begin{align*}
        \delta(x) &= \lim_{\epsilon \to 0} \frac{1}{\sqrt{2 \pi} \epsilon} e^{- x^2/ 2 \epsilon^2}
    \end{align*}
    Limit of Lorentzian functions:
    \begin{align*}
        \delta(x) &= \lim_{\epsilon \to 0} \frac{1}{\pi} \frac{\epsilon}{t^2 + \epsilon^2}
    \end{align*}
\end{frame}

\note{
\begin{itemize}
\item
    Two examples of delta function limits.
\item
    Confirms our intuition of the delta function as a limit of sharply-peaked functions.
\item
    In fact, basically any limit of sharply-peaked functions of area 1 will work: see Folland (Theorem 9.2).
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Delta function limits}
    A more interesting example:
    \begin{align*}
        \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{j x t} \d t &= \delta(x)
    \end{align*}
    because
    \begin{align*}
        \lim_{\epsilon \to 0} \frac{1}{2 \pi} \int_{-\infty}^{\infty} e^{-\epsilon^2 t^2} e^{j x t} \d t = \delta(x)
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Example of a common, but unintuitive expression for the Delta function.
    \item
        Can show that it's true by expressing it as a delta function limit. (If you want to go through it, use Theorem 9.2 from Folland.)
    \end{itemize}
    
}
\fi

\begin{frame}[fragile]
    \frametitle{What does this mean for Green's functions?}
    
    \begin{align*}
        \L G(x, x') &= \delta(x - x')
    \end{align*}
    actually means
    \begin{align*}
        \left( \L G \right)[\phi]  &= \phi(x') \stackrel{s}{=} \int_{-\infty}^\infty \left( \L G(x, x')\right) \phi(x) \d x
    \end{align*}
\end{frame}

\note{
\begin{itemize}
\item
    Technically, the Green's function is a generalized function such that $ \L G $ is the delta function (it has the sifting property).
\item
    In practise, we'll keep using the less-precise way; just remember that there is a more correct way.
\end{itemize}

}

\begin{frame}[fragile]
    \frametitle{Takeaway}
    
    \begin{center}
        If in doubt, think of $ \delta(x - x_0) $ as an operator, not a function!
    \end{center}

\end{frame}

\note{
\begin{itemize}
\item
    In practise, thinking of $ \delta(x - x_0) $ as a function is usually fine.
    (We'll even do that for the rest of this presentation.)
\item
    But if anything starts to seem fishy, it's good to remember that $ \delta(x - x_0) $ is actually an operator, and not a function.
\end{itemize}
    
}

\section{Direct solution}
\label{sec:direct_solution}

\note{
    \begin{itemize}
    \item
        Back to Green's functions!
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    Original problem:
    \begin{align*}
        \frac{\d^2 u(x)}{\d x^2} - k^2 u(x) &= f(x)
    \end{align*}
    
    Green's function problem:
    \begin{align*}
        \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') &= \delta(x - x')
    \end{align*}
    

\end{frame}

\note{
\begin{itemize}
\item
    Let's look at a simple example now.
\item
    This problem is similar to a simple harmonic oscillator, but the negative sign means we expect lossy behaviour rather than oscillation.
\item
    We won't worry much about boundary conditions yet, we'll just look for solutions that don't blow up at $ x = \pm \infty $.
\item
    If we can find the Green's function, then we can find the solution to the original problem.
\item
    But the Green's function problem looks pretty hard. The point of this example is to demonstrate that we can actually solve it.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    For $ x \neq x' $
    \begin{align*}
        \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') &= 0
    \end{align*}
    
    \pause
    So we have
    \begin{align*}
        G(x,x') &= 
        \begin{cases} 
            A e^{+k (x - x')} & \text{for } x < x'
            \\
            B e^{-k (x - x')} & \text{for } x > x'
        \end{cases}
    \end{align*}
    
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Key thing to notice is that the source is concentrated at $ x = x' $.
    \item
        So for $ x > x' $ and $ x < x' $, we expect the solutions to look like those of the source-free equation.
    \item
        To keep the solutions finite, we expect exponential growth before $ x = x' $ and exponential decay afterward.
    \item
        Now, how do we find the constants $ A $ and $ B $?
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    \begin{align*}
        \frac{\d^2 G(x,x')}{\d x^2} - k^2 G(x,x') &= \delta(x - x')
    \end{align*}

    Continuity of the Green's function:
    \begin{align*}
        \lim_{\epsilon \to 0} \left[ G(x'+\epsilon, x') - G(x' - \epsilon, x') \right] = 0
    \end{align*}

\end{frame}

\note{
\begin{itemize}
\item
    How continuous do we expect our Green's function to be?
\item
    If $ G(x,x') $ is discontinuous (like a step function), then $ \d G / \d x $ will behave like a delta function and $ \d^2 G / \d x^2 $ will behave like a delta function derivative. No good!
\item
    So we expect $ G(x,x') $ to be continuous.
\item
    That gives us one condition we can use to find $ A $ and $ B $. (In fact, it tells us that $ A = B $.)
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}
 
    \begin{align*}
        \alt<2->{\int_{x'-\epsilon}^{x'+\epsilon} \left[ \frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') \right] \d x &= \int_{x'-\epsilon}^{x'+\epsilon} \delta(x - x') \d x}{\frac{\d^2 G(x, x')}{\d x^2} - k^2 G(x,x') = \delta(x - x')}
    \end{align*}

    Discontinuity condition:
    \begin{align*}
        \lim_{\epsilon \to 0} \left[ \left. \frac{\d G}{\d x}\right|_{x = x' + \epsilon} - \left. \frac{\d G}{\d x} \right|_{x = x' - \epsilon} \right] &= 1
    \end{align*}
    

\end{frame}

\note{
    \begin{itemize}
    \item
        But what if the derivative $ \d G / \d x $ is discontinuous?
    \item
        Then $ \d^2 G / \d x^2 $ is like a delta function.
        But that's fine, because we have a delta function on the right hand side too.
    \item
        We can find exactly how discontinuous the derivative is by integrating over a small interval around $ x' $.
    \item
        In the limit of $ \epsilon \to 0 $, the second integral vanishes because $ G(x,x') $ is continuous.
    \item
        But, we expect $ \d G / \d x $ to be discontinous. 
    \item
        Using fundamental theorem of calculus, we get at a \emph{discontinuity condition for the derivative.} (Key idea for the direct solution method!)
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}

    \begin{align*}
        G(x,x') &= 
        \begin{cases} 
            A e^{+k (x - x')} & \text{for } x < x'
            \\
            B e^{-k (x - x')} & \text{for } x > x'
        \end{cases}
    \end{align*}
    
    Continuity of $ G $:
    \begin{align*}
        A &= B
    \end{align*}

    Discontinuity of $ \frac{\d G}{\d x} $:
    \begin{align*}
        k A + k B &= 1
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Applying our two conditions, we can solve for $ A $ and $ B $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{A simple example}
    At last, our Green's function is
    \begin{align*}
        G(x,x') &= 
        \begin{dcases} 
            \dfrac{e^{+k (x - x')}}{2k} & \text{for } x < x'
            \\
            \dfrac{e^{-k (x - x')}}{2k} & \text{for } x > x'
        \end{dcases}
    \end{align*}
    or, more compactly
    \begin{empheq}[box=\widefbox]{align*}
        G(x, x') = \frac{e^{k |x - x'|}}{2 k}
    \end{empheq}
    

\end{frame}

\note{
}

\begin{frame}[fragile]
    \frametitle{A simple example}
    Solution:
    \begin{align*}
        u(x) &= \int_{-\infty}^{\infty} f(x') \frac{e^{k|x - x'|}}{2k} \d x'
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Now that we have the Green's function, we can construct the solution to our original problem for any forcing function $ f(t) $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{General approach}
    Properties of $ G(x,x') $:
    \begin{itemize}
    \item
        Behaves like source-free solution except at $ x = x' $.
    \item
        Function is continuous at $ x = x' $.
    \item
        Derivative is discontinuous at $ x = x' $.
    \end{itemize}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Listed are the key things to note from that example.
    \item
        This approach works quite well for solving 1D Green's function problems.
    \end{itemize}
}

\section{Boundary conditions}
\label{sec:boundary_conditions}

\note{
    \begin{itemize}
    \item
        We didn't worry about boundary conditions in the last example.
    \item
        As it turns out, Green's functions allow us to deal with boundary conditions in an elegant way.
    \item
        Unfortunately, to derive it, we either have to do a lot of hand-waving or a lot of math. 
        We're going to do a lot of math.
    \item
        See Dudley's \textit{Mathematical foundations for electromagnetic theory} for a more thorough discussion.
    \end{itemize}
}

%\begin{frame}[fragile]
%    \frametitle{Sturm Liouville Problems}
%    
%    Differential equation:
%    \begin{align*}
%        \underbrace{\left( - \frac{1}{w(x)} \frac{\d}{\d x} \left[ p(x) \frac{\d}{\d x} \right] + q(x) - \lambda \right)}_{\displaystyle \L_\lambda} u(x) &= f(x)
%    \end{align*}
%    Boundary conditions:
%    \begin{align*}
%        B_1(u) &= \alpha_{11} u(a) + \alpha_{12} u'(a) + \alpha_{13} u(b) + \alpha_{14} u'(b) =  \alpha
%        \\
%        B_2(u) &= \alpha_{21} u(a) + \alpha_{22} u'(a) + \alpha_{23} u(b) + \alpha_{24} u'(b) =  \beta
%    \end{align*}
%    
%\end{frame}
%
%\note{
%    \begin{itemize}
%    \item
%        This is a general Sturm-Liouville problem.
%    \end{itemize}
%    
%}

\begin{frame}[fragile]
    \frametitle{Adjoint operators}

    \begin{IEEEeqnarray*}{rCrClCrCl}
        \text{Original:} & \qquad & \L [u(x)] &=& f(x); &\qquad& \B [u(x)] &=& 0
        \\
        \text{Adjoint:} & \qquad & \L^* [v(x)] &=& f(x); &\qquad& \B^* [v(x)] &=& 0
    \end{IEEEeqnarray*}
    
    Defining property:
    \begin{align*}
        \inprod{\L u}{v} = \inprod{u}{\L^* v} \qquad \text{where } \inprod{u}{v} = \int_{a}^{b} u(x) v^*(x) \d x
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        To really make sense of boundary conditions, we need the concept of the adjoint problem.
    \item
        Suppose we have an original problem defined by operator $ \L $ and boundary conditions $ \B $.
    \item
        Then, $ \L^* $ is the adjoint operator and $ \B^* $ are the adjoint boundary conditions if $ \inprod{\L u}{v} = \inprod{u}{\L^* v} $ for all $ u, v $.
    \item
        Here $ \inprod{u}{v} $ is the inner product as defined on the slide. ($ v^*(x) $ is the complex conjugate of $ v(x) $.)
    \item
        (Side note: technically, we should call $ \L^* $ the \emph{formal} adjoint. 
        A boundary condition like $ \B[u] = 0 $ restricts $ u $ to a subspace of its original Hilbert space. 
        Thus, if $ \B \neq \B^* $, the domains of $ \L $ and $ \L^* $ are not the same, and they are not true adjoints.)
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    \begin{align*}
        \L[u(x)] &= \left[ \frac{\d^2}{\d x^2} + k^2 \right] u(x)
        \\
        \B[u(x)] &= \mat{u(a) \\ u(b)} = \mat{0 \\ 0}
    \end{align*}

    Want $ \L^* $ and $ \B^* $ so that
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v}
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Let's look at an example: the 1D simple harmonic oscillator.
    \item
        We'll use boundary conditions so that $ u(a) = u(b) = 0 $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    
    \vspace{-36pt}
    \begin{align*}
        \inprod{\L u}{v} &= \int_a^b \left[ u''(x) + k^2 u(x) \right] v^*(x) \d x
        %\pause
        %\\
        %&= \int_a^b \left[ - u'(x) v^{*\prime}(x) + k^2 u(x) v^*(x) \right] \d x + \left[ u'(x) v^*(x) \right]_{a}^{b}
        \pause
        \\
        &= \int_a^b \left[ v^{\prime\prime *}(x) + k^2 v^*(x) \right] u(x) \d x + 
        \\& \quad + \left[ u'(x) v^*(x)  - u(x) v^{\prime *}(x) \right]_{a}^{b}
    \end{align*}
    By inspection:
    \begin{align*}
        \L^* v(x) &= \left[ \frac{\d^2}{\d x^2} + (k^2)^* \right] v(x)
    \end{align*}
    

\end{frame}

\note{
    \begin{itemize}
    \item
        Integrate by parts twice. 
    \item
        The remaining integral term looks like
        \begin{align*}
            \int_a^b u(x) \left[ \L^* v(x) \right]^* \d x 
        \end{align*}
        so we can read off $ \L^* $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
    
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + \left[ u'(x) v^*(x)  - u(x) v^{*\prime}(x) \right]_{a}^{b}
    \end{align*}

    Since $ u(a) = u(b) = 0 $,
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + u'(a) v^*(a) - u'(b) v^*(b)
    \end{align*}

    So, pick
    \begin{align*}
        v(a) = v(b) = 0
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        We almost have the required $ \inprod{\L u}{v} = \inprod{u}{\L^* v} $, but we need the part on the right (called the conjunct) to be zero.
    \item
        From the original problem, we have
        \begin{align*}
            \B[u] &= \mat{u(a) \\ u(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        To make the conjunct zero, we need the adjoint boundary conditions to be
        \begin{align*}
            \B^*[v] &= \mat{v(a) \\ v(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        So in this case, $ \B = \B^* $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators: example}
     
    What if $ u(a) = u'(a) = 0 $ (initial conditions)?
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + \left[ u'(x) v^*(x)  - u(x) v^{*\prime}(x) \right]_{a}^{b}
        \\
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + u'(b) v^*(b)  - u(b) v^{\prime *}(b)
    \end{align*}
    We must have \emph{final} conditions for $ v $:
    \begin{align*}
        v(b) = v'(b) = 0
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        What if we use the same operator $ \L $, but we switch from a boundary value problem to an initial condition problem?
        That is,
        \begin{align*}
            \B[u] &= \mat{u(a) \\ u'(a)} = \mat{0 \\ 0}
        \end{align*}
    \item
        Then, to make the conjunct zero, we need the adjoint boundary conditions to be
        \begin{align*}
            \B[v] &= \mat{v(b) \\ v'(b)} = \mat{0 \\ 0}
        \end{align*}
    \item
        For an initial condition problem, the adjoint problem is a \emph{final} condition problem!
        $ \B \neq \B^* $. 
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint operators}

    In general:
    \begin{align*}
        \inprod{\L u}{v} &= \inprod{u}{\L^* v} + J(u,v)
    \end{align*}
    where $ J(u,v) $ depends on $ u, v, u', v', \ldots $ at the boundaries.

    \begin{align*}
        \B[u] = \B^*[v] = 0 \Longleftrightarrow J(u,v) = 0
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    In that example, we saw that we always had $ \inprod{\L u}{v} $ equal to $ \inprod{u}{\L^* v} $ plus a ``leftover'' term called the conjunct.
\item
    This is true in general: we always have a leftover term $ J(u,v) $ which depends only on the boundary values of $ u $ and $ v $.
\item
    We say $ \B[u] = 0 $ and $ \B^*[v] = 0 $ are adjoint boundary conditions if and only if they make $ J(u,v) $.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Adjoint Green's functions}
 
    Original problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L [u(x)] &=& f(x); &\qquad& \B [u(x)] &=& \alpha 
    \end{IEEEeqnarray*}
    Green's problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L [G(x,x')] &=& \delta(x - x'); &\qquad& \B [G(x,x')] &=& 0
    \end{IEEEeqnarray*}
    Adjoint Green's problem:
    \begin{IEEEeqnarray*}{rClCrCl}
        \L^* [H(x,x')] &=& \delta(x - x'); &\qquad& \B^* [H(x,x')] &=& 0
    \end{IEEEeqnarray*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Now we'll be able to deal with boundary conditions properly.
\item
    We define $ G(x,x') $ to obey the same equation as $ u(x) $, but with $ f(x) \to \delta(x - x') $ and $ \alpha \to 0 $.
    As before, $ G(x,x') $ is the impulse response.
\item
    In addition, we define a new function $ H(x,x') $ which is called the adjoint Green's function.
    It obeys the adjoint version of the $ G(x,x') $ equation.
\item
    Warning: a lot of textbooks don't distinguish between $ H(x,x') $ and $ G(x,x') $.
    Quite often, the ``Green's function'' is really the adjoint Green's function.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    Original problem:
    \begin{align*}
        \L[u] = \frac{\d^2 u(x)}{\d x^2} + k^2 u(x) &= f(x); \qquad \mat{u(a) \\ u'(a)} = \mat{\alpha \\ \beta}
    \end{align*}

    Adjoint Green's problem:
    \begin{align*}
        \frac{\d^2 H(x,x')}{\d x^2} + k^2 H(x,x') &= \delta(x - x'); \quad \mat{H(b,x') \\ H'(b,x')} = \mat{0 \\ 0}
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We'll show how $ H(x,x') $ is useful with an example.
\item
    Here we have a driven simple harmonic oscillator with initial conditions.
    (We'll take $ k $ real for simplicity.)
\item
    From before, we know that the adjoint problem will be the same differential equation, but with final conditions instead of initial conditions.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                A \cos(k(x - x')) + B \sin(k(x - x')) & \text{for } x < x'
                \\
                C \cos(k(x - x')) + D \sin(k(x - x')) & \text{for } x > x'
            \end{cases}
    \end{align*}
    \begin{align*}
        \text{Final conditions } & \Longrightarrow C = D = 0
        \\
        \text{Continuity of function } & \Longrightarrow A = 0
        \\
        \text{Discontinuity of derivative } & \Longrightarrow B = \frac{-1}{k}
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        We can solve for $ H(x,x') $ using a similar approach to before.
    \item
        Except at $ x = x' $, we write $ H(x,x') $ as a solution to the source-free equation.
    \item
        Then we find the coefficients using the boundary conditions and continuity/discontinuity requirements.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                - k^{-1} \sin(k (x - x')) & \text{for } x < x'
                \\
                0 & \text{for } x > x'
            \end{cases} 
    \end{align*}
 
\end{frame}

\note{
    \begin{itemize}
    \item
        So we have our adjoint Green's function.
    \item
        Now we just need to figure out how to construct $ u(x) $ from it.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        \inprod{\L u(x)}{H(x,x')} &= \inprod{u(x)}{\L^* H(x,x')} + J(u, H)
        \\
        \inprod{f(x)}{H(x,x')} &= \inprod{u(x)}{\delta(x - x')} + J(u, H)
        \\
        \int_a^b f(x) H^*(x,x') \d x &= \int_a^b u(x) \delta(x - x') \d x + J(u, H)
    \end{align*}
    \begin{empheq}[box=\widefbox]{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x - J(u, H)
    \end{empheq}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        To construct the solution $ u(x) $, we take an inner product of $ \L u(x) $ with $ H(x,x') $, and apply our knowledge of adjoints and conjuncts.
    \item
        We arrive at a fairly general formula which looks close to what we expect a Green's function formula to look like, but with an extra conjunct term.
    \item
        We'll gain insight into the $ J(u,H) $ term by expanding it for this example.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
    Expand $ J(u,H) $:
    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x - \left[ \frac{\d u(x)}{\d x} H^*(x,x') - u(x) \frac{\d H^*(x,x')}{\d x} \right]_a^b
    \end{align*}

    Recall:
    \begin{align*}
        \mat{u(a) \\ u'(a)} = \mat{\alpha \\ \beta}; \qquad \mat{H(b,x') \\ H'(b,x')} = \mat{0 \\ 0}
    \end{align*}
    
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Now let's expand $ J(u,v) $ for our particular example. (We can basically copy it from a previous part of the derivation.)
    \item
        We can simplify the conjunct by remembering our boundary conditions.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}

    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x + \beta H^*(a,x') - \alpha \frac{\d H^*(a,x')}{\d x}
    \end{align*}
   
\end{frame}

\note{
    \begin{itemize}
    \item
        We're very close to our final goal. 
    \item
        The last thing will be to get rid of $ H(x,x') $ and replace it with $ G(x,x') $.
    \end{itemize}
    
}

\begin{frame}[fragile]
    \frametitle{Adjoint Green's functions}

    How are $ G(x,x') $ and $ H(x,x') $ related?
    \begin{align*}
        \inprod{\L G(x,x')}{H(x,x'')} &= \inprod{G(x,x')}{\L^* H(x,x'')}
        \pause
        \\
        \inprod{\delta(x - x')}{H(x,x'')} &= \inprod{G(x,x')}{\delta(x - x'')}
        \pause
        \\
        H^*(x',x'') &= G(x'',x')
    \end{align*}
        \pause
    \begin{empheq}[box=\widefbox]{align*}
        G(x,x') &= H^*(x',x)
    \end{empheq}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        Using the definition of the adjoint problem, we find that there is a simple relationship between $ G(x,x') $ and $ H(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
     
    \begin{align*}
        H(x,x') &= 
            \begin{cases} 
                - k^{-1} \sin(k (x - x')) & \text{for } x < x'
                \\
                0 & \text{for } x > x'
            \end{cases} 
    \end{align*}

    \begin{align*}
        G(x,x') &= 
            \begin{cases} 
                0 & \text{for } x < x'
                \\
                k^{-1} \sin(k (x - x')) & \text{for } x > x'
            \end{cases} 
    \end{align*}

\end{frame}

\note{
    \begin{itemize}
    \item
        Since we already know the adjoint Green's function $ H(x,x') $, we can use find the Green's function via $ G(x,x') = H^*(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Example with boundary conditions}
 
    \begin{align*}
        u(x') &= \int_a^b f(x) H^*(x,x') \d x + \beta H^*(a,x') - \alpha \left. \frac{\d H^*(x,x')}{\d x} \right|_{x=a}
        \pause
        \\
        u(x') &= \int_a^b f(x) G(x',x) \d x + \beta G(x',a) - \alpha \left. \frac{\d G(x',x)}{\d x} \right|_{x=a}
        \pause
        \\
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
    \end{align*}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        With our new knowledge that $ G(x,x') = H^*(x',x) $, we can rewrite the solution $ u(x) $ in terms of $ G(x,x') $.
    \end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Final solution}

    \begin{empheq}[box=\widefbox]{align*}
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
    \end{empheq}
    where
    \begin{align*}
        G(x,x') &= 
            \begin{cases} 
                0 & \text{for } x < x'
                \\
                k^{-1} \sin(k (x - x')) & \text{for } x > x'
            \end{cases} 
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    Finally, we arrive at our solution.
\item
    First, note that the final answer does \emph{not} depend on $ H(x,x') $. We didn't actually need to ever calculate $ H(x,x') $ from the adjoint Green's equation, we could have just found $ G(x,x') $ from the (non-adjoint) Green's equation.
\item
    However, it would have been very difficult to derive this expression without using $ H(x,x') $ (I couldn't see an easy way).
    Because of this, a lot of authors stop at the expression for $ u(x') $ in terms of $ H^*(x,x') $, and they just call $ H^*(x,x') $ the ``Green's function.''
\item
    By doing the extra work, though, we gain a very nice interpretation for the boundary conditions term.
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Interpretation of boundary conditions}
    
    \begin{align*}
        u(x) &= \int_a^b f(x') G(x,x') \d x' + \beta G(x,a) - \alpha \left.\frac{\d G(x,x')}{\d x'}\right|_{x'=a}
        \\
        u(x) &= \int_a^b \Big[ f(x') + \beta \delta(x' - a) - \alpha \delta'(x' - a) \Big] G(x,x') \d x'
    \end{align*}
    
\end{frame}

\note{
\begin{itemize}
\item
    We note that our expression for $ u(x) $ looks like it did before (integral over $ f(x') G(x,x') $), but now there are extra terms which depend on the boundary conditions.
\item
    We come to a key idea: boundary conditions have the same effect on $ u(x) $ as adding little impulse sources at the boundary.
    The Green's function can deal with both sources $ f(x) $ and non-zero boundary conditions.
\item
    Be careful, though: $ G(x,x') $ still depends on the \emph{type} of boundary condition.
    E.g., we use the same $ G(x,x') $ for all initial value problems ($ u(a), u'(a) $ specified), but we'll need a different $ G(x,x') $ for boundary value problems ($ u(a), u(b) $ specified).
\end{itemize}
}

\begin{frame}[fragile]
    \frametitle{Summary}
    \vspace{-36pt}
    \begin{IEEEeqnarray*}{rClCrCl}
        \L[u(x)] &=& f(x); &\qquad& \B[u(x)] &=& \alpha
        \\
        \L[G(x,x')] &=& \delta(x - x'); &\qquad& \B[G(x,x')] &=& 0
        \\
        \L^*[H(x,x')] &=& \delta(x - x'); &\qquad& \B^*[H(x,x')] &=& 0
    \end{IEEEeqnarray*}

    \begin{enumerate}
    \item
        Find $ G(x,x') $.
    \item
        Find $ u(x') $ in terms of $ H(x,x') $:
        \begin{align*}
            u(x') &= \int_a^b f(x) H^*(x,x') \d x - J(u(x), H(x,x'))
        \end{align*}
    \item
        Express $ u(x) $ in terms of $ G(x,x')= H^*(x',x) $.
    \end{enumerate}
    
\end{frame}

\note{
    \begin{itemize}
    \item
        That was a long process, so let's summarize what we did.
    \item
        First, we wrote out the original equation, the Green's function equation, and the adjoint Green's function equation. (Take note of the boundary conditions in particular.)
    \item
        Next, we solve the Green's function equation for $ G(x,x') $.
    \item
        Next, we found that it's much easier to express $ u(x) $ in terms of $ H(x,x') $, because we can use inner products.
        The only tricky part is finding the adjoint (usually, just use integration by parts).
        See Dudley for a general formula for Sturm-Liouville problems.
    \item
        Finally, we replace $ H(x,x') $ with $ G^*(x',x) $ (which we solved for previously), and we have our final expression for $ u(x) $.
    \end{itemize}
}

%
%\begin{frame}[fragile]
%    \frametitle{Simple harmonic oscillator}
%    
%    Original problem:
%    \begin{align*}
%        \frac{\d^2 u(x)}{\d x^2} + \omega_0^2 u(x) &= f(x)
%    \end{align*}
%    $ u(a) $ and $ u(b) $ are known.
%
%    ``Adjoint'' Green's function problem:
%    \begin{align*}
%        \frac{\d^2 H(x,x')}{\d x^2} + \omega_0^2 H(x,x') &= \delta(x - x')
%    \end{align*}
%    
%
%\end{frame}
%
%\note{
%    \begin{itemize}
%    \item
%        Now let's look at an example with boundary conditions: a simple harmonic oscillator.
%    \item
%        To deal with this, we have to introduce the adjoint Green's function equation.
%    \item
%        In this case, the adjoint Green's function $ H(x,x') $ follows the same equation as the Green's function $ G(x,x') $, but it will have different boundary conditions.
%    \item
%        We want to find a good set of boundary conditions for $ H(x,x') $.
%    \end{itemize}
%}
%
%\begin{frame}[fragile]
%    \frametitle{Simple harmonic oscillator}
%
%    \begin{align*}
%        \alt<2>{G(x,x') \left[ \frac{\d^2 u(x')}{\d x^{\prime 2}} + \omega_0^2 u(x') \right] &= G(x,x') f(x')}{\frac{\d^2 u(x')}{\d x^{\prime 2}} + \omega_0^2 u(x') &= f(x')}
%    \end{align*}
%
%    \begin{align*}
%        \alt<2>{u(x') \left[ \frac{\d^2 G(x',x)}{\d x^{\prime 2}} + \omega_0^2 G(x',x) \right] &= u(x') \delta(x' - x)}{\frac{\d^2 G(x',x)}{\d x^{\prime 2}} + \omega_0^2 G(x',x) &= \delta(x' - x)}
%    \end{align*}
%
%\end{frame}
%
%\note{
%    \begin{itemize}
%    \item
%        To figure out the best boundary conditions, we use a little bit of trickery.
%    \item
%        First, we replace $ x \longleftrightarrow x' $ because it will make the final answer work out more nicely.
%    \item
%        Then we multiply the original equation by $ G(x,x') $ and the Green's functions equation with by $ u(x) $.
%    \end{itemize}
%}
%
%\begin{frame}[fragile]
%    \frametitle{Simple harmonic oscillator}
%  
%    \begin{align*}
%        \alt<2>{ \int_a^b \big[ u(x') \delta(x' - x) &- G(x,x') f(x') \big] \d x' }{  u(x') \delta(x' - x) - G(x,x') f(x') = u(x') \frac{\d^2 G(x',x)}{\d x^{\prime 2}} - G(x,x') \frac{\d^2 u(x')}{\d x^{\prime 2}}}
%        \alt<2>{\\&= \int_a^b \left[ u(x') \frac{\d^2 G(x',x)}{\d x^{\prime 2}} - G(x,x') \frac{\d^2 u(x')}{\d x^{\prime 2}} \right] \d x' }{}
%    \end{align*}
%   
%\end{frame}
%
%\note{
%\begin{itemize}
%\item
%    Next, we subtract the two equations and integrate $ x' $ over $ [a,b] $.
%\item
%    The integral over $ u(x') \delta(x' - x) $ becomes just $ u(x) $.
%\item
%    The right hand side can be dealt with using integration by parts.
%\end{itemize}
%}
%
%\begin{frame}[fragile]
%    \frametitle{Simple harmonic oscillator}
%
%    \begin{align*}
%        u(x) &= \int_a^b G(x,x') f(x') \d x' + \left[ u(x') \frac{\d G(x',x)}{\d x'} - G(x,x') \frac{\d u(x')}{\d x'} \right]_{x'=a}^b
%    \end{align*}
%
%    \pause
%    If we set $ G(a,x) = G(b,x) = 0 $, then
%    \begin{align*}
%        u(x) &= \int_a^b G(x,x') f(x') \d x' + 
%        \\
%        & \quad + u(b) \left. \frac{\d G(x',x)}{\d x'} \right|_{x' = b} - u(a) \left. \frac{\d G(x',x)}{\d x'} \right|_{x' = a}
%    \end{align*}
%
%\end{frame}
%
%\note{
%\begin{itemize}
%\item
%    We now have a more informative expression for $ u(x) $.
%\item
%    We have the original integral over $ G(x,x') f(x') $, but now we also have a boundary term.
%    (Be careful! $ x $ and $ x' $ are switched in one place.)
%\item
%    Problem: the last term requires us to know $ u $ and $ \d u/\d x $ at both $ x = a $ and $ x = b $.
%    But we only know $ u $ at those points, not $ \d u / \d x $.
%\item
%    Tricky solution: we haven't picked the boundary conditions for $ G(x,x') $ yet, so why not set them to
%    \begin{align*}
%        G(a,x') = G(b,x') = 0
%    \end{align*}
%    Then we don't need to know $ \d u/\d x $ on the boundary anymore.
%\end{itemize}
%}
%
%\begin{frame}[fragile]
%    \frametitle{Simple harmonic oscillator}
% 
%    What if we knew $ u(a) $ and $ \frac{\d u(a)}{\d x} $?
%    \begin{align*}
%        u(x') &= \int_a^b G(x,x') f(x) \d x + \left[ u(x) \frac{\d G(x,x')}{\d x} - G(x,x') \frac{\d u(x)}{\d x} \right]_{x=a}^b
%    \end{align*}
% 
%    \pause
%    Then set $ G(b,x') = \frac{\d G(b,x')}{\d x} = 0 $, so that
%    \begin{align*}
%        u(x') &= \int_a^b G(x,x') f(x) \d x + u(a) \frac{\d G(a, x')}{\d x} + G(a,x') \frac{\d u(a)}{\d x}
%    \end{align*}
%   
%\end{frame}
%
%\note{
%\begin{itemize}
%\item
%    What if we had an initial condition problem instead of a boundary value problem?
%\item
%    This time, we don't know $ u(b) $ or $ \frac{\d u(b)}{\d x} $, so we set $ G(b,x') = \frac{\d G(b,x')}{\d x} = 0 $ to eliminate them.
%\item
%    Note: the \emph{initial} value problem for $ u(x) $ corresponds to a \emph{final} value problem for $ G(x,x') $.
%\item
%    In general, the boundary conditions of $ G(x,x') $ are \emph{not} the same as those of $ u(x)  $: we pick them to eliminate the unknowns on the right hand side of the $ u(x') $ equation.
%\item
%    Boundary conditions for $ G(x,x') $ are called the ``adjoint'' boundary conditions.
%\end{itemize}
%}
%
%\begin{frame}[fragile]
%    \frametitle{The adjoint problem}
%
%\end{frame}
%
%\note{}

\section{Spectral methods}
\label{sec:spectral_methods}

\note{}

\section{3D problems}
\label{sec:3d_problems}

\note{}

\section{Properties of the Green's function}
\label{sec:properties_of_the_green_s_function}

\note{}

\section{Advanced topics}
\label{sec:advanced_topics}

\note{}


\begin{frame}[fragile]
    \frametitle{Introductory resources}
    Balanis (2012), \emph{Advanced engineering electromagnetics}. 
    Less rigorous, but good for getting the key ideas.

    Folland (1992), \emph{Fourier analysis and its applications}. 
    Chapter on generalized functions is particularly nice.

    Dudley (1994), \emph{Mathematical foundations for electromagnetic theory}.
    Great introduction to 1D Green's functions: deals with subtleties that others ignore.

    Byron and Fuller (1992), \emph{Mathematics of classical and quantum physics}.
    Interesting alternative approach.
    
\end{frame}

\note{}

\begin{frame}[fragile]
    \frametitle{Advanced resources}
    Collin (1990), \emph{Field theory of guided waves}. 
    Huge chapter on Green's functions. Emphasis on dyadics.

    Morse and Feshback, \emph{Methods of theoretical physics}.
    Another big, detailed reference. Emphasis on theory and insights.

    Warnick (1996), ``Electromagnetic Green functions using differential forms.''
    For the differential forms inclined.

\end{frame}    

\note{}

\end{document}
